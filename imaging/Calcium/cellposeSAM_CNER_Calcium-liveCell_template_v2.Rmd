---
title: "cellposeSAM_CNER_Calcium-liveCell_template_v2"
output: html_document
date: "2025-07-16"
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# I. Load packackes
```{r}
library(devtools)

# Core data manipulation & visualization
library(tidyverse)        # ggplot2, dplyr, purrr, readr, tibble, tidyr, stringr
library(zoo)              # Rolling operations
library(reshape2)         # Data reshaping
library(pheatmap)         # Heatmaps
library(ggdendro)         # Dendrograms
library(viridis)          # Color palettes
library(RColorBrewer)
library(NatParksPalettes)
library(tidyplots)
library(ggpubr)           # Publication-ready plots
library(ggsignif)         # Significance annotations
library(Cairo)            # For cairo_pdf output
library(patchwork)        # Plot layouts
library(cowplot)          # Plot composition
library(gridExtra)        # Layout utilities
library(grid)             # Viewports and custom layout
library(rlang)            # Tidy evaluation tools

# Network analysis
library(igraph)
library(ggraph)
#devtools::install_github("simo-91/CalNetExploreR")
library(CalNetExploreR)

# Signal processing
#library(signal)

# Plot composition
library(gridExtra)
library(grid)
library(cowplot)
library(RColorBrewer)

# PCA visualization
library(factoextra)

library(dplyr)

```

# II. Preprocessing & Analysis
=> cellposeSAM based evaluation %>% into batch dF/F processing script 
=> used cellposeSAM for image segementation and measurement of ROIs
=> cellposeSAM output:
- each row is one soma followed by mean intensities saved across columns 
- each new timepoint is new column 

## IIa. User Input & Configuration
```{r}
# ==========================================================
# CellposeSAM-based Calcium Imaging Batch Analysis Pipeline
# ==========================================================
# This script integrates output from CellposeSAM, processes dF/F traces, 
# binarizes events, computes correlation-based networks (CNER), and 
# extracts per-cell statistics.

# -----------------------------------------
# USER INPUT & OUTDIRS 
# -----------------------------------------
# Path to root directory where input data and output folders are/will be stored
#base_path <- "/Users/felix/HMS Dropbox/Felix Kraus/Felix/Harvard/03_LSD-PD/Microscopy/20250623_diff138_iN_d21_Ctrl-ASAH1_Flou4/cpsam_d21/nd2_labels/"
base_path <- "/Users/felix/HMS Dropbox/Felix Kraus/Felix/Harvard/03_LSD-PD/Microscopy/20250623_diff138_iN_d21_Ctrl-ASAH1_Flou4/cpsam_d21/test2/"

# Subfolder with merged per-object intensity values from CellProfiler
input_file <- file.path(base_path, "combined_intensity_df.csv")

# Subfolder with per-object centroid coordinates (used for network overlay)
centroid_file <- file.path(base_path, "combined_centroids_df.csv")

# Path to save all plots
ca_results_dir <- file.path(base_path, "calcium_analysis_results")
dir.create(ca_results_dir, showWarnings = FALSE, recursive = TRUE)

# define output dirs:
ca_results_dataframes <- file.path(base_path, "calcium_analysis_results_dataframes")
dir.create(ca_results_dataframes, recursive = TRUE, showWarnings = FALSE)

# -------------------
# SAMPLE INFO & COLOR MAPPING 
# -------------------

# USER INPUT HERE 
celltype <- "iN"
timepoint <- "d21"
genotypes <- c("Ctrl", "ASAH1")
treatments <- c("Fed_Fluo4", "100mMKCL_Fluo4", "10µMCQNX_Fluo4")



# Generate genotype_map
sample_keys <- expand.grid(
  CellType  = celltype,
  Timepoint = timepoint,
  Genotype  = genotypes,
  Treatment = treatments,
  stringsAsFactors = FALSE
) %>%
  mutate(
    Key   = paste(CellType, Timepoint, Genotype, Treatment, sep = "_"),
    Label = paste(CellType, Timepoint, Genotype, Treatment, sep = "_")  # customize here if needed
  )

genotype_map <- setNames(sample_keys$Label, sample_keys$Key)

# Colors auto-generated based on mapping
geno_levels <- unique(unname(genotype_map))
base_colors <- RColorBrewer::brewer.pal(length(geno_levels), "Set2")  
geno_cols <- setNames(base_colors, geno_levels)

stopifnot(length(base_colors) >= length(geno_levels))


# -------------------
# ANALYSIS PARAMETERS
# -------------------
baseline_window <- 100             # Rolling window size for baseline (frames). should be maybe 10% of frames to get good avg.
frame_rate      <- 10
smoothing_k     <- NULL            # Optional smoothing over raw intensity traces (frames), set to NULL to disable smoothing
threshold_factor <- 0.4            # Threshold for binarization (fraction of SD)
correlation_threshold <- 0.6       # Correlation threshold for network (used in make_network())
roi_corr_threshold    <- 0.4       # for identifying synchronized ROIs (used in spike rate filtering)
image_dim <- c(1570, 1140)         # width, height in pixels of image. Need to backmap the roi activity network onto the image coordinats 

```


## IIb. parse through folder, make meta file list, add raw data and save 
```{r}
# -----------------------------
# Helper Function – parse through folder read in files and name file colum
# -----------------------------

# --- 1. Parse and annotate metadata
# List all files
all_files <- list.files(base_path, full.names = TRUE)

# Select files
intensity_files <- all_files[str_detect(all_files, "_intensity\\.csv$")]
centroids_files <- all_files[str_detect(all_files, "_centroids_valid\\.csv$")]
intensity_meta <- parse_metadata(intensity_files)
centroids_meta <- parse_metadata(centroids_files)


# Metadata parser
parse_metadata <- function(file_list) {
  tibble(File = file_list) %>%
    mutate(
      Filename  = basename(File),
      File_base = str_remove(Filename, "_(intensity|centroids_valid)\\.csv$"),
      parts     = str_split(File_base, "_", simplify = TRUE),
      nparts    = ncol(parts),
      Celltype  = parts[, 1],
      Timepoint = parts[, 2],
      Genotype  = parts[, 3],
      Treatment = ifelse(nparts == 5, parts[, 4], paste(parts[, 4], parts[, 5], sep = "_")),
      Replicate = ifelse(nparts == 6, parts[, 6], ifelse(nparts == 5, parts[, 5], NA_character_)),
      Sample    = str_c(Celltype, Timepoint, Genotype, Treatment, sep = "_"),
      FileID    = str_c(Celltype, Timepoint, Genotype, Treatment, Replicate, sep = "_")
    ) %>%
    select(File, Sample, FileID, Celltype, Timepoint, Genotype, Treatment, Replicate)
}



# --- 2. Read and merge metadata with CSVs
read_and_annotate_csv <- function(meta_df, is_centroid = FALSE) {
  map_dfr(1:nrow(meta_df), function(i) {
    df <- readr::read_csv(meta_df$File[i], show_col_types = FALSE)
    
    if (is_centroid && "label" %in% names(df)) {
      df <- df %>% rename(ObjectNumber = label)
    }

    meta_info <- meta_df[i, ] %>% select(-File)
    bind_cols(meta_info[rep(1, nrow(df)), ], df)
  })
}

# Final annotated data frames
intensity_df <- read_and_annotate_csv(intensity_meta)
centroids_df <- read_and_annotate_csv(centroids_meta, is_centroid = TRUE)
intensity_df <- intensity_df %>%
  rename(frame = time)

write.csv(intensity_df, file = file.path(ca_results_dataframes, "intensity_df.csv"), row.names = FALSE)
write.csv(centroids_df, file = file.path(ca_results_dataframes, "centroids_df.csv"), row.names = FALSE)

```


## IIc. baseline correction, ∆F/F Processing
```{r}
# -----------------------------
# Helper Function – ΔF/F Calculation
# -----------------------------
calculate_dff <- function(trace, window = baseline_window, smoothing_k = NULL, percentile = 0.2) {
  if (!is.null(smoothing_k)) {
    trace <- zoo::rollmean(trace, k = smoothing_k, fill = NA)
  }

  if (length(trace) < window || all(is.na(trace))) {
    return(rep(NA, length(trace)))
  }

  baseline <- zoo::rollapply(trace, width = window,
                              FUN = function(x) quantile(x, probs = percentile, na.rm = TRUE),
                              align = "center", fill = NA, partial = TRUE)

  baseline <- pmax(baseline, 1e-3)  # Avoid division by zero
  dff <- (trace - baseline) / baseline
  pmax(dff, 0)  # Clip negatives
}

# -----------------------------
# Helper Function – Noise Filter + dF/F from in-memory data
# -----------------------------
process_pair <- function(file_id, intensity_df, centroids_df) {
  intensity_sub <- intensity_df %>% filter(FileID == file_id)
  centroids_sub <- centroids_df %>% filter(FileID == file_id)

  if (nrow(intensity_sub) == 0 || nrow(centroids_sub) == 0) return(NULL)

  intensity_matrix <- intensity_sub %>%
    select(where(is.numeric), -frame)

  baseline_avg <- median(as.numeric(intensity_matrix[1, ]), na.rm = TRUE)
  filter_thresh <- 0.3 * baseline_avg

  intensity_filtered <- intensity_sub %>%
    mutate(across(where(is.numeric) & !matches("frame"), ~pmax(.x, filter_thresh)))

  traces_dff <- intensity_filtered %>%
    mutate(across(where(is.numeric) & !matches("frame"), ~calculate_dff(.x, window = baseline_window, smoothing_k = smoothing_k)))

traces_long <- traces_dff %>%
  mutate(ImageNumber = row_number(), FileID = file_id) %>%
  pivot_longer(
    cols = matches("^[0-9]+$"),
    names_to = "ObjectNumber",
    values_to = "Intensity_IntegratedIntensity_Fura",
    values_drop_na = TRUE
  ) %>%
  mutate(
    ObjectNumber = as.integer(ObjectNumber)
  ) %>%
  left_join(centroids_sub, by = c("ObjectNumber", "FileID")) %>%
  left_join(distinct(intensity_sub, FileID, Sample, Celltype, Timepoint, Genotype, Treatment, Replicate), by = "FileID")


  return(traces_long)
}

# -----------------------------
# Run dF/F processing across all samples
# -----------------------------
dff_results <- map(unique(intensity_df$FileID), ~process_pair(.x, intensity_df, centroids_df)) %>%
  compact() %>%
  bind_rows()

write.csv(dff_results, file = file.path(ca_results_dataframes, "dff_results.csv"), row.names = FALSE)


# -----------------------------
# Initialize Result Lists
# -----------------------------
cor_list <- list()
comm_list <- list()
events_list <- list()
summary_list <- list()
```


## IId. Trace Plotting (per sample), Correlations, Network plots and Firing Statistics 
```{r}
# -------------------------------------------------
# Trace Plotting (per sample), Correlations, Network plots and Firing Statistics 
# -------------------------------------------------

# --- Setup
trace_dir <- file.path(base_path, "label_traces")
dir.create(trace_dir, recursive = TRUE, showWarnings = FALSE)

all_sample_ids <- unique(combined_df$file)
centroid_df_clean <- centroid_df %>%
  dplyr::mutate(file = as.character(file), ObjectNumber = as.integer(ObjectNumber))

# --- Palettes
corr_palette <- colorRampPalette(rev(RColorBrewer::brewer.pal(11, "RdYlBu")))(100)
node_palette <- colorRampPalette(rev(RColorBrewer::brewer.pal(9, "YlGnBu")))(100)

for (sample_id in all_sample_ids) {
  clean_sample_id <- gsub("_centroids_valid.csv$", "", basename(sample_id))
  output_dir <- file.path(base_path, clean_sample_id)
  dir.create(output_dir, recursive = TRUE, showWarnings = FALSE)

  # --- dF/F computation
  df_sample <- dplyr::filter(combined_df, file == sample_id) %>%
    dplyr::select(-matches("^centroid_")) %>%
    dplyr::left_join(centroid_df_clean, by = c("file", "ObjectNumber")) %>%
    dplyr::group_by(ObjectNumber) %>%
    dplyr::arrange(ImageNumber, .by_group = TRUE) %>%
    dplyr::mutate(
      dF_F         = calculate_dff(Intensity_IntegratedIntensity_Fura, window = baseline_window, smoothing_k = smoothing_k, percentile = 0.4),
      dF_F_scaled  = dF_F / max(dF_F, na.rm = TRUE),
      dF_F_binary  = ifelse(dF_F > threshold_factor * sd(dF_F, na.rm = TRUE), 1, 0),
      spike_amplitude = ifelse(dF_F_binary == 1, dF_F_scaled, NA)
    ) %>%
    dplyr::ungroup()

  # --- ROI trace plots
  roi_ids <- unique(df_sample$ObjectNumber)
  for (i in seq_along(roi_ids)) {
    df_trace <- dplyr::filter(df_sample, ObjectNumber == roi_ids[i])
    p_raw    <- ggplot(df_trace, aes(x = ImageNumber, y = Intensity_IntegratedIntensity_Fura)) + geom_line() + theme_minimal()
    p_scaled <- ggplot(df_trace, aes(x = ImageNumber, y = dF_F_scaled)) + geom_line(color = "dodgerblue") + theme_minimal()
    p_binary <- ggplot(df_trace, aes(x = ImageNumber, y = dF_F_binary)) + geom_step(color = "firebrick2") + theme_minimal()
    ggsave(file.path(trace_dir, paste0(clean_sample_id, "_ROI_", i, ".pdf")),
           p_raw / p_scaled / p_binary, width = 6, height = 6, device = cairo_pdf)
  }

  # --- Binary matrix and correlation matrix
  mat <- df_sample %>%
    dplyr::select(ObjectNumber, ImageNumber, dF_F_binary) %>%
    tidyr::pivot_wider(names_from = ImageNumber, values_from = dF_F_binary) %>%
    dplyr::arrange(ObjectNumber)
  mat_matrix <- as.matrix(dplyr::select(mat, -ObjectNumber))
  rownames(mat_matrix) <- mat$ObjectNumber
  cor_mat <- cor(t(mat_matrix), use = "pairwise.complete.obs")
  cor_mat[!is.finite(cor_mat)] <- 0

  pdf(file.path(output_dir, paste0(clean_sample_id, "_CorrelationMatrix.pdf")), width = 6, height = 6)
  pheatmap(cor_mat, clustering_method = "complete", main = "Correlation Matrix", border_color = NA, fontsize = 6)
  dev.off()

  # --- Identify synchronized ROIs based on roi_corr_threshold
  roi_edge_idx <- which(cor_mat > roi_corr_threshold & upper.tri(cor_mat), arr.ind = TRUE)
  roi_high_corr_ids <- unique(c(rownames(cor_mat)[roi_edge_idx[, 1]], rownames(cor_mat)[roi_edge_idx[, 2]]))

  # --- Group label from map
  group_label <- if (sample_id %in% names(genotype_map)) genotype_map[[sample_id]] else "Unknown"

  # --- Filter short/long bursts
  df_sample <- df_sample %>%
    dplyr::group_by(ObjectNumber) %>%
    dplyr::arrange(ImageNumber, .by_group = TRUE) %>%
    dplyr::mutate(
      run_id   = data.table::rleid(dF_F_binary),
      run_len  = ave(dF_F_binary, ObjectNumber, run_id, FUN = length),
      dF_F_binary = ifelse(dF_F_binary == 1 & (run_len < 1 | run_len > 60), 0, dF_F_binary)
    ) %>%
    dplyr::ungroup()

  # --- Recompute rising edge
  df_sample <- df_sample %>%
    dplyr::group_by(ObjectNumber) %>%
    dplyr::arrange(ImageNumber, .by_group = TRUE) %>%
    dplyr::mutate(rising_edge = dF_F_binary == 1 & lag(dF_F_binary, default = 0) == 0) %>%
    dplyr::ungroup()

  # --- Tag synchronized ROIs
  df_sample <- df_sample %>%
    dplyr::mutate(is_high_corr = as.character(ObjectNumber) %in% roi_high_corr_ids)

  # --- Firing stats (by ROI)
  firing_stats <- df_sample %>%
    dplyr::group_by(ObjectNumber, is_high_corr) %>%
    dplyr::summarise(
      firing_events   = sum(rising_edge, na.rm = TRUE),
      total_frames    = dplyr::n(),
      active_frames   = sum(dF_F_binary, na.rm = TRUE),
      percent_active  = 100 * active_frames / total_frames,
      spike_amplitude = if (sum(dF_F_binary == 1 & dF_F_scaled > 0.2, na.rm = TRUE) > 0) {
        quantile(dF_F_scaled[dF_F_binary == 1 & dF_F_scaled > 0.2], 0.75, na.rm = TRUE)
      } else {
        NA_real_
      },
      .groups = "drop"
    ) %>%
    dplyr::mutate(
      Sample = sample_id,
      firing_rate_per_min = firing_events / (max(df_sample$ImageNumber, na.rm = TRUE) / (frame_rate * 60)),
      Group = group_label
    )
  events_list[[clean_sample_id]] <- firing_stats

  # --- Sample-level summary
  n_cells        <- dplyr::n_distinct(df_sample$ObjectNumber)
  n_frames_total <- max(df_sample$ImageNumber, na.rm = TRUE)
  duration_min   <- n_frames_total / (frame_rate * 60)

  sample_summary <- firing_stats %>%
    dplyr::summarise(
      Sample                          = clean_sample_id,
      Total_cells                     = n_cells,
      Total_firing_events             = sum(firing_events, na.rm = TRUE),
      Firing_events_per_min           = Total_firing_events / duration_min,
      Firing_events_per_min_norm      = Firing_events_per_min / n_cells,
      Cells_active                    = sum(firing_events > 0, na.rm = TRUE),
      Percent_cells_active            = 100 * Cells_active / n_cells,
      Mean_percent_active_frames      = mean(percent_active, na.rm = TRUE),
      Mean_percent_active_frames_norm = Mean_percent_active_frames
    )
  summary_list[[clean_sample_id]] <- sample_summary

  # --- Correlation-based network edges
  edge_list <- which(cor_mat > correlation_threshold & upper.tri(cor_mat), arr.ind = TRUE)
  edges <- data.frame(
    from = rownames(cor_mat)[edge_list[, 1]],
    to   = rownames(cor_mat)[edge_list[, 2]],
    corr = cor_mat[edge_list]
  )

  valid_corr_vals <- cor_mat[upper.tri(cor_mat) & is.finite(cor_mat)]
  min_corr <- min(valid_corr_vals, na.rm = TRUE)
  max_corr <- 1  # fixed upper bound

  edges$corr_scaled <- scales::rescale(edges$corr, to = c(1, 100), from = c(min_corr, max_corr))
  edges$corr_scaled <- pmin(pmax(round(edges$corr_scaled), 1), 100)
  edges$color <- corr_palette[edges$corr_scaled]
  write.csv(edges, file.path(output_dir, "edge_list_with_colors.csv"), row.names = FALSE)

  # --- Coordinates and node coloring
  coords <- df_sample %>%
    dplyr::filter(!is.na(centroid_x), !is.na(centroid_y)) %>%
    dplyr::group_by(ObjectNumber) %>%
    dplyr::slice(1) %>%
    dplyr::ungroup() %>%
    dplyr::distinct(ObjectNumber, centroid_x, centroid_y) %>%
    dplyr::mutate(Label = as.character(ObjectNumber))

  spike_rates <- df_sample %>%
    dplyr::group_by(ObjectNumber) %>%
    dplyr::summarise(spike_rate = mean(dF_F_binary, na.rm = TRUE)) %>%
    dplyr::mutate(Label = as.character(ObjectNumber))

  coords <- dplyr::left_join(coords, spike_rates, by = "Label") %>%
    dplyr::mutate(
      color_idx = pmin(pmax(round(scales::rescale(spike_rate, to = c(1, 100))), 1), 100),
      color = node_palette[color_idx]
    )

  # --- Network plotting wrapper
  plot_network <- function(node_colors, edge_colors, legend_title, legend_range, file_name) {
    pdf(file.path(output_dir, file_name), width = 6.9, height = 5)
    layout(matrix(c(1, 2), nrow = 1), widths = c(5, 1.5))
    par(mar = c(4, 4, 2, 1))
    plot(coords$centroid_x, coords$centroid_y,
         pch = 21, bg = node_colors, col = "grey50",
         xlab = "X", ylab = "Y", main = clean_sample_id,
         asp = 1, xlim = c(0, image_dim[1]), ylim = c(0, image_dim[2]))
    for (i in seq_len(nrow(edges))) {
      p1 <- dplyr::filter(coords, Label == edges$from[i])
      p2 <- dplyr::filter(coords, Label == edges$to[i])
      if (nrow(p1) == 1 && nrow(p2) == 1) {
        segments(p1$centroid_x, p1$centroid_y, p2$centroid_x, p2$centroid_y,
                 col = edge_colors[i], lwd = 1)
      }
    }
    par(mar = c(4, 2, 2, 4))
    legend_img <- as.raster(matrix(rev(corr_palette), ncol = 1))
    plot(NA, xlim = c(0, 1), ylim = legend_range, type = "n", axes = FALSE, xlab = "", ylab = "")
    rasterImage(legend_img, 0, legend_range[1], 1, legend_range[2])
    axis(4, at = pretty(legend_range), las = 1)
    mtext(legend_title, side = 4, line = 2.5)
    dev.off()
  }

  # --- Network visualizations
  plot_network(rep("grey50", nrow(coords)), edges$color, "Correlation", c(min_corr, max_corr), "network_from_centroids.pdf")
  plot_network(coords$color, edges$color, "Spike rate", c(min_corr, max_corr), "network_from_centroids_spikerate.pdf")

  # --- Save processed data
  write.csv(filter(firing_stats, is_high_corr == TRUE),  file.path(output_dir, paste0(clean_sample_id, "_firing_stats_highcorr.csv")), row.names = FALSE)
  write.csv(filter(firing_stats, is_high_corr == FALSE), file.path(output_dir, paste0(clean_sample_id, "_firing_stats_lowcorr.csv")),  row.names = FALSE)
  write.csv(df_sample, file.path(output_dir, paste0(clean_sample_id, "_df_processed.csv")), row.names = FALSE)

  # --- Save correlation matrix (long)
  cor_long <- as.data.frame(as.table(cor_mat)) %>%
    dplyr::rename(from = Var1, to = Var2, corr = Freq) %>%
    dplyr::mutate(Sample = sample_id) %>%
    dplyr::filter(from != to)
  cor_list[[clean_sample_id]] <- cor_long

  # --- Louvain community detection
  g <- igraph::graph_from_data_frame(edges, directed = FALSE)
  if (igraph::gorder(g) > 0 && igraph::gsize(g) > 0) {
    comms <- igraph::cluster_louvain(g)
    membership <- membership(comms)
    community_sizes <- sizes(comms)

    membership_df <- data.frame(
      ObjectNumber   = names(membership),
      Community      = membership,
      Sample         = sample_id,
      CommunitySize  = as.integer(community_sizes[as.character(membership)])
    )
    comm_list[[clean_sample_id]] <- membership_df
  }
}




#dev.off()


```





# III.  Plotting & Statistical Evaluation
## IIIa. ROI Couting and Event Rates Calculation 
```{r}
## ---------------------------------------------------------------------------
## Plot CellposeSAM / CNER results — incl. events per min⁻¹ (±SEM)
## ---------------------------------------------------------------------------

# Helper to bind lists with Sample column
bind_with_sample <- function(lst) {
  imap_dfr(lst, ~ {
    if (!"Sample" %in% names(.x))
      .x <- mutate(.x, Sample = .y)
    .x
  })
}

# Function to standardize sample naming
clean_sample_names <- function(s) {
  s %>%
    stringr::str_replace("_centroids_valid\\.csv$", "") %>%
    stringr::str_replace("_\\d+$", "")
}

# Collapse lists to data frames
events_df <- bind_with_sample(events_list)
comm_df   <- bind_with_sample(comm_list)

# Add Label column to comm_df for later joins
comm_df <- comm_df %>%
  mutate(Label = as.character(ObjectNumber))

# Sanity check: Sample column exists
stopifnot("Sample" %in% names(events_df), "Sample" %in% names(comm_df))

# ----------------------------------------------------------------------
# Standardize Sample column and assign Genotype
# ----------------------------------------------------------------------

# events_df: ROI-level
events_df <- events_df %>%
  mutate(
    Sample = clean_sample_names(as.character(Sample)),
    Label  = as.character(ObjectNumber),
    Genotype = genotype_map[Sample]
  )

# comm_df: ROI-level
comm_df <- comm_df %>%
  mutate(
    Sample = clean_sample_names(as.character(Sample)),
    Label  = as.character(ObjectNumber),
    Genotype = genotype_map[Sample]
  )

# normalised_metrics: sample-level
normalised_metrics <- normalised_metrics %>%
  mutate(
    Sample   = clean_sample_names(as.character(Sample)),
    Genotype = genotype_map[Sample]
  )

# ----------------------------------------------------------------------
# ROI count and calculation of (un)normalized firing rates
# ----------------------------------------------------------------------
events_df <- events_df %>%
  group_by(Sample) %>%
  mutate(
    n_labels             = n(),  # total ROIs in sample
    events_per_min       = firing_events / (total_frames / (frame_rate * 60)),
    events_per_min_norm  = events_per_min / n_labels
  ) %>%
  ungroup()

# ----------------------------------------------------------------------
# Add is_high_corr to normalised_metrics after aggregation
# ----------------------------------------------------------------------
events_corr <- events_df %>%
  select(Sample, is_high_corr) %>%
  distinct()

normalised_metrics <- normalised_metrics %>%
  left_join(events_corr, by = "Sample")

# ----------------------------------------------------------------------
# Set genotype-specific colors
# ----------------------------------------------------------------------
unique_genos <- unique(na.omit(c(events_df$Genotype, comm_df$Genotype)))
geno_cols <- setNames(colorRampPalette(base_colors)(length(unique_genos)), unique_genos)

```


## IIIb. Violin plots with Kruskal-Wallis test 
```{r}
# ----------------------------------------------------------
# Helper: Violin plot with significance stats
# ----------------------------------------------------------
plot_genotype_violin_with_stats <- function(df, metric, ylab, output_prefix, base_dir = ca_results_dir,
                                            add_sem = FALSE, jitter_color = "grey50") {
  require(ggpubr)

  metric_sym <- rlang::sym(metric)

  # Ensure correct genotype order
  valid_levels <- intersect(unique(unname(genotype_map)), unique(df$Genotype))
  df <- df %>%
    mutate(Genotype = factor(Genotype, levels = valid_levels))

  # Add early exit if only one genotype
  if (length(unique(na.omit(df$Genotype))) < 2) {
    message("Skipping plot: Only one genotype present in the data subset.")
    return(NULL)
  }
  
  # Kruskal-Wallis
  kruskal_pval <- kruskal.test(reformulate("Genotype", metric), data = df)$p.value

  # Pairwise Wilcoxon
  pw <- pairwise.wilcox.test(df[[metric]], df$Genotype, p.adjust.method = "holm")
  pairwise_df <- as.data.frame(as.table(pw$p.value), stringsAsFactors = FALSE) %>%
    dplyr::rename(group1 = Var1, group2 = Var2, p.value = Freq) %>%
    dplyr::filter(!is.na(p.value)) %>%
    dplyr::mutate(
      significance = case_when(
        p.value < 0.001 ~ "***",
        p.value < 0.01  ~ "**",
        p.value < 0.05  ~ "*",
        TRUE            ~ NA_character_
      ),
      y.position = seq(from = max(df[[metric]], na.rm = TRUE) * 1.05,
                       by = max(df[[metric]], na.rm = TRUE) * 0.05,
                       length.out = dplyr::n())
    ) %>%
    filter(!is.na(significance))

  # Mean ± SEM if requested
  if (add_sem) {
    mean_df <- df %>%
      group_by(Genotype) %>%
      summarise(
        mean_val = mean(!!metric_sym, na.rm = TRUE),
        sem      = sd(!!metric_sym, na.rm = TRUE) / sqrt(n()),
        .groups = "drop"
      )
  }

  # Plot
  p <- ggplot(df, aes(x = Genotype, y = !!metric_sym, fill = Genotype)) +
    geom_violin(scale = "width", trim = FALSE, color = NA, width = 0.85) +
    geom_boxplot(width = 0.2, fill = "white", outlier.shape = NA, linewidth = 0.3) +
    geom_jitter(width = 0.1, size = 0.5, alpha = 0.5, color = jitter_color) +
    scale_fill_manual(values = geno_cols) +
    labs(y = ylab, x = NULL) +
    theme_bw(base_size = 6) +
    theme(panel.grid = element_blank(), legend.position = "none", aspect.ratio = 1.5,axis.text.x = element_text(angle = 90, hjust = 0.5))

  if (nrow(pairwise_df) > 0) {
    p <- p + stat_pvalue_manual(pairwise_df,
                                label = "significance",
                                y.position = "y.position",
                                xmin = "group1", xmax = "group2",
                                tip.length = 0.01, size = 2)
  }

  ggsave(file.path(base_dir, paste0(output_prefix, "_violin.pdf")),
         p, width = 3, height = 5, device = cairo_pdf)

  # Return
  kruskal_df <- tibble(
    group1 = "Kruskal-Wallis", group2 = "",
    p.value = kruskal_pval,
    significance = case_when(
      kruskal_pval < 0.001 ~ "***",
      kruskal_pval < 0.01  ~ "**",
      kruskal_pval < 0.05  ~ "*",
      TRUE                 ~ "ns"
    )
  )

  return(list(p = p, stats = bind_rows(kruskal_df, pairwise_df)))
}
```


## IIIc. pairwise Wilcoxon tests
```{r}
# ----------------------------------------------------------
# Run comparisons
# ----------------------------------------------------------

# Split events_df by ROI correlation
events_high <- events_df %>% filter(is_high_corr == TRUE)
events_low  <- events_df %>% filter(is_high_corr == FALSE)

# Per-cell metrics
res_high      <- plot_genotype_violin_with_stats(events_high, "firing_events",   "Firing Events",    "firing_highcorr")
res_low       <- plot_genotype_violin_with_stats(events_low,  "firing_events",   "Firing Events",    "firing_lowcorr")
res_high_amp  <- plot_genotype_violin_with_stats(events_high, "spike_amplitude", "Spike Amplitude",  "amp_highcorr")
res_low_amp   <- plot_genotype_violin_with_stats(events_low,  "spike_amplitude", "Spike Amplitude",  "amp_lowcorr")

# Sample-level normalized
res_fire_norm <- plot_genotype_violin_with_stats(normalised_metrics, "Firing_events_per_min_norm",
                                                 "Events · min⁻¹ / ROI", "firing_norm")
res_active_norm <- plot_genotype_violin_with_stats(normalised_metrics, "Mean_percent_active_frames_norm",
                                                   "% Active / ROI", "active_norm")
```


## IIId. Save combined Outputs & Sample-Level Normalized Outputs
```{r}
# ------------------------------------------------------
# Save Combined Outputs (Cell-Pair Correlations, Communities, Events)
# ------------------------------------------------------
write.csv(bind_rows(cor_list),    file.path(ca_results_dataframes, "combined_cell_pairwise_correlations.csv"), row.names = FALSE)
write.csv(bind_rows(comm_list),   file.path(ca_results_dataframes, "combined_cell_community_membership.csv"),  row.names = FALSE)
write.csv(bind_rows(events_list), file.path(ca_results_dataframes, "combined_events_per_min_results.csv"),     row.names = FALSE)

# ------------------------------------------------------
# Save sample-level normalized metrics (already annotated with genotype)
# ------------------------------------------------------
write.csv(normalised_metrics,
          file.path(ca_results_dataframes, "combined_sample_level_normalised_metrics.csv"),
          row.names = FALSE)

```


## IIIe. Sample-Level Normalized Metrics
```{r}
# ---------------------------------------------------------------------------
# Sample‑level normalised metrics from `normalised_metrics`
# ---------------------------------------------------------------------------

# plot low / high corr filtered violin metrics
# firing rate 
p_fire_norm_result <- plot_genotype_violin_with_stats(
  normalised_metrics,
  metric = "Firing_events_per_min_norm",
  ylab = "Events · min⁻¹ / ROI",
  output_prefix = "fire_events_norm",
  base_dir = ca_results_dir
)
p_active_norm_result <- plot_genotype_violin_with_stats(
  normalised_metrics,
  metric = "Mean_percent_active_frames_norm",
  ylab = "% Active / ROI",
  output_prefix = "percent_active_norm",
  base_dir = ca_results_dir
)


# plot low / high corr filtered firing rates 
p_fire_norm_high_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == TRUE),
  metric = "events_per_min_norm",
  ylab = "Events · min⁻¹ / ROI",
  output_prefix = "fire_events_norm_high",
  base_dir = ca_results_dir
)
p_fire_norm_low_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == FALSE),
  metric = "events_per_min_norm",
  ylab = "Events · min⁻¹ / ROI",
  output_prefix = "fire_events_norm_low",
  base_dir = ca_results_dir
)

# plot low / high active frames
p_active_norm_high_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == TRUE),
  metric = "percent_active",
  ylab = "% Active / ROI",
  output_prefix = "percent_active_norm_high",
  base_dir = ca_results_dir
)
p_active_norm_high_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == TRUE),
  metric = "percent_active",
  ylab = "% Active / ROI",
  output_prefix = "percent_active_norm_high",
  base_dir = ca_results_dir
)

# plot low / high spike amplitude  
p_amp_orig_high_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == TRUE),
  metric = "spike_amplitude",
  ylab = "Spike Amplitude",
  output_prefix = "spike_amplitude_high",
  base_dir = ca_results_dir
)
p_amp_orig_low_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == FALSE),
  metric = "spike_amplitude",
  ylab = "Spike Amplitude",
  output_prefix = "spike_amplitude_low",
  base_dir = ca_results_dir
)


# Extract violin plots
p_fire_norm         <- p_fire_norm_result$p
p_active_norm       <- p_active_norm_result$p

p_fire_norm_high    <- p_fire_norm_high_results$p
p_fire_norm_low     <- p_fire_norm_low_results$p

p_active_norm_high  <- p_active_norm_high_results$p
p_active_norm_low   <- p_active_norm_low_results$p

p_amp_orig_high     <- p_amp_orig_high_results$p
p_amp_orig_low      <- p_amp_orig_low_results$p

```



## IIIf. Community Size Analysis (Non-parametric) 
```{r}
# ----------------------------------------------------------------------
# Ensure genotype is annotated
# ----------------------------------------------------------------------
# After computing comm_df and events_df and converting types
comm_df <- comm_df %>%
  mutate(Sample = as.character(Sample),
         Label  = as.character(ObjectNumber))

events_df <- events_df %>%
  mutate(Sample = as.character(Sample),
         Label  = as.character(ObjectNumber))

# Add is_high_corr to comm_df
comm_df <- left_join(
  comm_df,
  events_df %>% select(Sample, Label, is_high_corr),
  by = c("Sample", "Label")
)

# now annotate Genotype
comm_df <- add_genotype(comm_df, genotype_map)



# ----------------------------------------------------------------------
# Stratify by high/low correlation
# ----------------------------------------------------------------------
comm_high <- comm_df %>% filter(is_high_corr == TRUE)
comm_low  <- comm_df %>% filter(is_high_corr == FALSE)

# ----------------------------------------------------------------------
# Helper to run KW + Wilcoxon + plot
# ----------------------------------------------------------------------
analyze_comm_size <- function(df, label, outfile_prefix) {
  # Skip if only one genotype
  if (length(unique(df$Genotype[!is.na(df$Genotype)])) < 2) {
    message("Skipping analysis: only one genotype present.")
    return(list(plot = NULL, stats = tibble()))
  }

  # Kruskal-Wallis test
  kw_pval <- kruskal.test(CommunitySize ~ Genotype, data = df)$p.value

  # Pairwise Wilcoxon
  pw <- pairwise.wilcox.test(df$CommunitySize, df$Genotype, p.adjust.method = "holm")
  pairwise_df <- as.data.frame(as.table(pw$p.value), stringsAsFactors = FALSE) %>%
    rename(group1 = Var1, group2 = Var2, p.value = Freq) %>%
    filter(!is.na(p.value)) %>%
    mutate(
      significance = case_when(
        p.value < 0.001 ~ "***",
        p.value < 0.01  ~ "**",
        p.value < 0.05  ~ "*",
        TRUE            ~ "ns"
      ),
      y.position = seq(from = max(df$CommunitySize, na.rm = TRUE) * 1.05,
                       by = max(df$CommunitySize, na.rm = TRUE) * 0.05,
                       length.out = dplyr::n()),
      group1 = factor(group1, levels = levels(df$Genotype)),
      group2 = factor(group2, levels = levels(df$Genotype))
    )

  # Plot
  p <- ggplot(df, aes(x = Genotype, y = CommunitySize, fill = Genotype)) +
    geom_jitter(color = "grey80", width = 0.1, size = 0.5, alpha = 0.5) +
    geom_violin(scale = "width", trim = FALSE, color = NA, width = 0.85, adjust = 1.5) +
    geom_boxplot(width = 0.2, fill = "white", outlier.shape = NA, color = "black", linewidth = 0.3) +
    scale_fill_manual(values = geno_cols) +
    labs(y = "Community Size", x = NULL, title = label) +
    theme_bw(base_size = 6) +
    theme(legend.position = "none",
          aspect.ratio    = 2,
          panel.grid = element_blank(),
          axis.text.x = element_text(angle = 90, hjust = 0.5))

  if (nrow(pairwise_df) > 0) {
    p <- p + stat_pvalue_manual(pairwise_df,
                                label = "significance",
                                y.position = "y.position",
                                xmin = "group1", xmax = "group2",
                                tip.length = 0.01, size = 2)
  }

  ggsave(file.path(ca_results_dir, paste0(outfile_prefix, ".pdf")),
         p, width = 2, height = 3, device = cairo_pdf)

  return(list(p = p,
              stats = bind_rows(
                tibble(group1 = "Kruskal-Wallis", group2 = "", p.value = kw_pval,
                       significance = case_when(
                         kw_pval < 0.001 ~ "***",
                         kw_pval < 0.01  ~ "**",
                         kw_pval < 0.05  ~ "*",
                         TRUE            ~ "ns")),
                pairwise_df)))
}

# ----------------------------------------------------------------------
# Run per-group comparisons
# ----------------------------------------------------------------------
res_comm_high <- analyze_comm_size(comm_high, "Community Size (High Corr ROIs)", "community_size_high")
res_comm_low  <- analyze_comm_size(comm_low,  "Community Size (Low Corr ROIs)",  "community_size_low")

# ----------------------------------------------------------------------
# Save combined stats
# ----------------------------------------------------------------------
stats_comm_all <- bind_rows(
  res_comm_high$stats %>% mutate(Source = "high_corr_ROIs"),
  res_comm_low$stats  %>% mutate(Source = "low_corr_ROIs")
)

write.csv(stats_comm_all,
          file = file.path(ca_results_dataframes, "community_size_stats_high_vs_low.csv"),
          row.names = FALSE)



# plots
comm_df <- add_genotype(comm_df, genotype_map)
p_comm_high <- plot_genotype_violin_with_stats(
  df = comm_df %>% filter(is_high_corr == TRUE),
  metric = "CommunitySize",
  ylab = "# Communities",
  output_prefix = "community_stats_high",
  base_dir = ca_results_dir
)

p_comm_low <- plot_genotype_violin_with_stats(
  df = comm_df %>% filter(is_high_corr == FALSE),
  metric = "CommunitySize",
  ylab = "# Communities",
  output_prefix = "community_stats_low",
  base_dir = ca_results_dir
)
```



## IIIg. Time-binned Firing Metrics Over Time
```{r}
# -------------------------------------------------
# Time-binned Firing Metrics Over Time (High vs Low Corr)
# -------------------------------------------------

# Add is_high_corr to centroid_df
centroid_df <- centroid_df %>%
  mutate(ObjectNumber = as.character(ObjectNumber))

corr_annot <- events_df %>%
  select(Sample, Label, is_high_corr) %>%
  mutate(Label = as.character(Label))

centroid_df <- left_join(centroid_df, corr_annot,
                         by = c("file" = "Sample", "ObjectNumber" = "Label"))

# Ensure ObjectNumber has matching types
combined_df  <- combined_df  %>% mutate(ObjectNumber = as.character(ObjectNumber))
centroid_df  <- centroid_df  %>% mutate(ObjectNumber = as.character(ObjectNumber))

# Add genotype, bin, and join metadata
traces_with_genotype <- combined_df %>%
  left_join(centroid_df, by = c("file", "ObjectNumber")) %>%
  mutate(
    Bin = floor(ImageNumber / baseline_window),
    Genotype = purrr::map_chr(file, function(s) {
      matched <- names(genotype_map)[str_detect(s, fixed(names(genotype_map)))]
      if (length(matched) > 0) genotype_map[[matched[1]]] else NA_character_
    })
  ) %>%
  filter(!is.na(Genotype), !is.na(is_high_corr))

# Calculate metrics per cell per bin
binned_metrics <- traces_with_genotype %>%
  group_by(file, ObjectNumber, Bin, Genotype, is_high_corr) %>%
  arrange(ImageNumber, .by_group = TRUE) %>%
  mutate(
    dF_F = calculate_dff(Intensity_IntegratedIntensity_Fura,
                         window = baseline_window,
                         smoothing_k = smoothing_k,
                         percentile = 0.2),
    dF_F_scaled = if (all(is.na(dF_F))) 0 else dF_F / max(dF_F, na.rm = TRUE),
    dF_F_binary = ifelse(dF_F > threshold_factor * sd(dF_F, na.rm = TRUE), 1, 0),
    spike_amplitude = ifelse(dF_F_binary == 1, dF_F_scaled, NA_real_)
  ) %>%
  mutate(rising_edge = dF_F_binary == 1 & lag(dF_F_binary, default = 0) == 0) %>%
  summarise(
    firing_events = sum(rising_edge, na.rm = TRUE),
    total_frames = n(),
    percent_active = 100 * sum(dF_F_binary, na.rm = TRUE) / total_frames,
    spike_amplitude = mean(spike_amplitude, na.rm = TRUE),
    .groups = "drop"
  )

# Summarize per genotype, bin, and corr group
plot_df_split <- binned_metrics %>%
  group_by(Genotype, Bin, is_high_corr) %>%
  summarise(
    mean_events = mean(firing_events, na.rm = TRUE),
    sem_events  = sd(firing_events, na.rm = TRUE) / sqrt(n()),
    mean_active = mean(percent_active, na.rm = TRUE),
    sem_active  = sd(percent_active, na.rm = TRUE) / sqrt(n()),
    mean_amp    = mean(spike_amplitude, na.rm = TRUE),
    sem_amp     = sd(spike_amplitude, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"
  )

# Plot helper
plot_timecourse <- function(data, metric, sem, ylab) {
  ggplot(data, aes(x = Bin * baseline_window, y = .data[[metric]], color = Genotype)) +
    geom_line(linewidth = 0.5) +
    geom_ribbon(aes(ymin = .data[[metric]] - .data[[sem]],
                    ymax = .data[[metric]] + .data[[sem]],
                    fill = Genotype), alpha = 0.3, color = NA) +
    labs(x = "Frame", y = ylab) +
    scale_color_manual(values = geno_cols) +
    scale_fill_manual(values = geno_cols) +
    theme_bw(base_size = 6) +
    theme(panel.grid = element_blank(),
          axis.text.x = element_text(angle = 90, hjust = 0.5),
          legend.position = "right")
}

# Generate and save plots by correlation group
for (corr_state in c(TRUE, FALSE)) {
  tag <- if (corr_state) "highcorr" else "lowcorr"
  df_sub <- filter(plot_df_split, is_high_corr == corr_state)

  p1 <- plot_timecourse(df_sub, "mean_events", "sem_events", "Firing Events")
  p2 <- plot_timecourse(df_sub, "mean_active", "sem_active", "% Active Frames")
  p3 <- plot_timecourse(df_sub, "mean_amp", "sem_amp", "Spike Amplitude")

  pdf(file.path(ca_results_dir, paste0("time_binned_metrics_", tag, ".pdf")), width = 6, height = 6)
  print(p1)
  print(p2)
  print(p3)
  dev.off()
}



# -------------------------------------------------
# plot low / high corr filtered time-binned metrics
# -------------------------------------------------
# filtered event rates 
p_bin_events_high <- plot_timecourse(
  plot_df_split %>% filter(is_high_corr == TRUE),
  "mean_events",
  "sem_events",
  "Firing Events"
)

p_bin_events_low <- plot_timecourse(
  plot_df_split %>% filter(is_high_corr == FALSE),
  "mean_events",
  "sem_events",
  "Firing Events"
)

# filtered activity rates 
p_bin_active_high <- plot_timecourse(
  plot_df_split %>% filter(is_high_corr == TRUE),
  "mean_active",
  "sem_active",
  "% Active Frames"
)

p_bin_active_low <- plot_timecourse(
  plot_df_split %>% filter(is_high_corr == FALSE),
  "mean_active",
  "sem_active",
  "% Active Frames"
)

# filtered spike amplitude 
p_bin_amp_high <- plot_timecourse(
  plot_df_split %>% filter(is_high_corr == TRUE),
  "mean_amp",
  "sem_amp",
  "Spike Amplitude"
)

p_bin_amp_low <- plot_timecourse(
  plot_df_split %>% filter(is_high_corr == FALSE),
  "mean_amp",
  "sem_amp",
  "Spike Amplitude"
)


# Extract time-binned plots
p_bin_events_high   <- p_bin_events_high
p_bin_events_low    <- p_bin_events_low

p_bin_active_high   <- p_bin_active_high
p_bin_active_low    <- p_bin_active_low

p_bin_amp_high      <- p_bin_amp_high
p_bin_amp_low       <- p_bin_amp_low

```


## IIIf. Combined Summary Figure & export of all values 
```{r}
# ---------------------------------------------------------------------------
# Create combined six‑panel figure (high and low corr separately)
# ---------------------------------------------------------------------------

# High correlation summary
neuro_combinedplot_high <-
  ((p_fire_norm_high | p_active_norm_high | p_amp_orig_high) /
   (p_bin_amp_high | p_bin_events_high | p_comm_high)) +
  plot_layout(heights = c(1, 1)) &
  theme(plot.margin = margin(5, 10, 5, 10))

ggsave(file.path(ca_results_dir, "neuro_combinedplot_high.pdf"),
       neuro_combinedplot_high, width = 12, height = 12, device = cairo_pdf)

# Low correlation summary
neuro_combinedplot_low <-
  ((p_fire_norm_low | p_active_norm_low | p_amp_orig_low) /
   (p_bin_amp_low | p_bin_events_low | p_comm_low)) +
  plot_layout(heights = c(1, 1)) &
  theme(plot.margin = margin(5, 10, 5, 10))

ggsave(file.path(ca_results_dir, "neuro_combinedplot_low.pdf"),
       neuro_combinedplot_low, width = 12, height = 12, device = cairo_pdf)


# ---------------------------------------------------------------------------
# Combine and save all nonparametric p-values
# ---------------------------------------------------------------------------
all_pvals_high <- bind_rows(
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == TRUE), "firing_rate_per_min"),
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == TRUE), "percent_active"),
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == TRUE), "spike_amplitude"),
  calc_nonparametric_stats(comm_df   %>% filter(is_high_corr == TRUE), "CommunitySize")
) %>% mutate(Source = "high_corr_ROIs")

all_pvals_low <- bind_rows(
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == FALSE), "firing_rate_per_min"),
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == FALSE), "percent_active"),
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == FALSE), "spike_amplitude"),
  calc_nonparametric_stats(comm_df   %>% filter(is_high_corr == FALSE), "CommunitySize")
) %>% mutate(Source = "low_corr_ROIs")

all_pvals_combined <- bind_rows(all_pvals_high, all_pvals_low)

write.csv(all_pvals_combined,
          file = file.path(ca_results_dataframes, "calcium_metrics_pvals_high_vs_low.csv"),
          row.names = FALSE)
```
