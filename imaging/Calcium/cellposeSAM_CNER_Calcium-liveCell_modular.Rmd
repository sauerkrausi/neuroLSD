---
title: "cellposeSAM_CNER_Calcium-liveCell_v2_diff138_d21"
output: html_document
date: "2025-07-17"
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# I. Load packackes
```{r}
library(devtools)

# Core data manipulation & visualization
library(tidyverse)        # ggplot2, dplyr, purrr, readr, tibble, tidyr, stringr
library(zoo)              # Rolling operations
library(reshape2)         # Data reshaping
library(pheatmap)         # Heatmaps
library(ggdendro)         # Dendrograms
library(viridis)          # Color palettes
library(RColorBrewer)
library(NatParksPalettes)
library(tidyplots)
library(ggpubr)           # Publication-ready plots
library(ggsignif)         # Significance annotations
library(Cairo)            # For cairo_pdf output
library(patchwork)        # Plot layouts
library(cowplot)          # Plot composition
library(gridExtra)        # Layout utilities
library(grid)             # Viewports and custom layout
library(rlang)            # Tidy evaluation tools

# Network analysis
library(igraph)
library(ggraph)
#devtools::install_github("simo-91/CalNetExploreR")
library(CalNetExploreR)

# Signal processing
#library(signal)
library(pracma) 

# Plot composition
library(gridExtra)
library(grid)
library(cowplot)
library(RColorBrewer)

# PCA visualization
library(factoextra)

library(dplyr)

```

# II. Preprocessing & Analysis
=> cellposeSAM based evaluation %>% into batch dF/F processing script 
=> used cellposeSAM for image segementation and measurement of ROIs
=> cellposeSAM output:
- each row is one soma followed by mean intensities saved across columns 
- each new timepoint is new column 

## IIa. User Input & Configuration
```{r}
# ==========================================================
# CellposeSAM-based Calcium Imaging Batch Analysis Pipeline
# ==========================================================
# This script integrates output from CellposeSAM, processes dF/F traces, 
# binarizes events, computes correlation-based networks (CNER), and 
# extracts per-cell statistics.

# -----------------------------------------
# USER INPUT & OUTDIRS 
# -----------------------------------------
# Path to root directory where input data and output folders are/will be stored
#base_path <- "/Users/felix/HMS Dropbox/Felix Kraus/Felix/Harvard/03_LSD-PD/Microscopy/20250623_diff138_iN_d21_Ctrl-ASAH1_Flou4/cpsam_d21/nd2_labels"
base_path <- "/Users/felix/HMS Dropbox/Felix Kraus/Felix/Harvard/03_LSD-PD/Microscopy/20250623_diff138_iN_d21_Ctrl-ASAH1_Flou4/cpsam_d21/test"

# Subfolder with merged per-object intensity values from CellProfiler
input_file <- file.path(base_path, "combined_intensity_df.csv")

# Subfolder with per-object centroid coordinates (used for network overlay)
centroid_file <- file.path(base_path, "combined_centroids_df.csv")

# Path to save all plots
ca_results_dir <- file.path(base_path, "calcium_analysis_results")
dir.create(ca_results_dir, showWarnings = FALSE, recursive = TRUE)

# define output dirs:
ca_results_dataframes <- file.path(base_path, "calcium_analysis_results_dataframes")
dir.create(ca_results_dataframes, recursive = TRUE, showWarnings = FALSE)



# -------------------
# SAMPLE INFO & COLOR MAPPING 
# -------------------

# USER INPUT HERE 
celltype <- "iN"
timepoint <- "d21"
genotypes <- c("Ctrl", "ASAH1")
treatments <- c("Fed_Fluo4", "100mMKCL_Fluo4", "10µMCQNX_Fluo4")

# -- define which treatments are stimulating / inhibitory 
stim_ref <- "100mMKCL"
inh_ref  <- "10µMCQNX"
basal_ref  <- "Fed"
basal_group <- "Ctrl_Fed_Fluo4"


## Add ordered label factor for consistent plotting
sample_keys <- expand.grid(
  CellType  = celltype,
  Timepoint = timepoint,
  Genotype  = genotypes,
  Treatment = treatments,
  stringsAsFactors = FALSE
) %>%
  mutate(
    Key   = paste(CellType, Timepoint, Genotype, Treatment, sep = "_"),
    Label = paste(CellType, Timepoint, Genotype, Treatment, sep = "_"),
    PlotOrder = paste(Genotype, match(Treatment, treatments), sep = "_")  # combine for sorting
  ) %>%
  arrange(factor(Genotype, levels = genotypes), match(Treatment, treatments)) %>%
  mutate(Label = factor(Label, levels = Label))  # ordered factor for plotting

genotype_map <- setNames(sample_keys$Label, sample_keys$Key)


# define function for proper genotype / sample order 
set_genotype_factor <- function(df) {
  df %>%
    mutate(Genotype = factor(Genotype, levels = levels(sample_keys$Label)))
}


# # Colors auto-generated based on mapping
geno_levels <- unique(unname(genotype_map))
base_colors <- RColorBrewer::brewer.pal(length(geno_levels), "Set2")  
 
# stopifnot(length(base_colors) >= length(geno_levels))

# Manually assign colors based on genotype identity
geno_cols <- setNames(
  sapply(geno_levels, function(label) {
    if (str_detect(label, "_Ctrl_")) {
      "grey70"
    } else if (str_detect(label, "_ASAH1_")) {
      "lightblue"
    } else {
      "lightgrey"  # fallback for unrecognized
    }
  }),
  geno_levels
)

# -------------------
# ANALYSIS PARAMETERS
# -------------------
baseline_window <- 50             # Rolling window size for baseline (frames). should be maybe 10% of frames to get good avg.
frame_rate      <- 10
smoothing_k     <- NULL            # Optional smoothing over raw intensity traces (frames), set to NULL to disable smoothing
#threshold_factor <- 0.4            # Threshold for binarization (fraction of SD)
correlation_threshold <- 0.4       # Correlation threshold for network (used in make_network())
roi_corr_threshold    <- 0.4       # for identifying synchronized ROIs (used in spike rate filtering)
image_dim <- c(1570, 1140)         # width, height in pixels of image. Need to backmap the roi activity network onto the image coordinats 

```


## IIb1. Helper Function – parse through paried intesnitsy / centroids and join in wide format
```{r}
# -----------------------------
# Helper Function – parse through paired intensity / centroids and join in wide format
# -----------------------------

# define output dirs:
ca_results_matched <- file.path(base_path, "01_matched_dfs")
dir.create(ca_results_matched, recursive = TRUE, showWarnings = FALSE)


# Find matching pairs by FileID
intensity_files <- list.files(base_path, pattern = "_intensity\\.csv$", full.names = TRUE)
centroid_files  <- list.files(base_path, pattern = "_centroids_valid\\.csv$", full.names = TRUE)

intensity_meta <- tibble(
  File = intensity_files,
  FileID = str_remove(basename(File), "_intensity\\.csv$")
)

centroids_meta <- tibble(
  File = centroid_files,
  FileID = str_remove(basename(File), "_centroids_valid\\.csv$")
)

# Process each matched pair
matched_ids <- intersect(intensity_meta$FileID, centroids_meta$FileID)

for (file_id in matched_ids) {
  message("Processing: ", file_id)

  # Load files
  int_file <- intensity_meta %>% filter(FileID == file_id) %>% pull(File)
  cen_file <- centroids_meta %>% filter(FileID == file_id) %>% pull(File)

  intensity_df <- read_csv(int_file, show_col_types = FALSE)
  centroids_df <- read_csv(cen_file, show_col_types = FALSE)

  if ("label" %in% names(centroids_df)) {
    centroids_df <- centroids_df %>% rename(ObjectNumber = label)
  }

  # Match ROI labels
  shared_labels <- intersect(
    colnames(intensity_df)[grepl("^[0-9]+$", colnames(intensity_df))],
    as.character(centroids_df$ObjectNumber)
  )
  if (length(shared_labels) == 0) next

  # Reshape intensity to wide format (rows = ROI, cols = time)
  intensity_long <- intensity_df %>%
    pivot_longer(cols = all_of(shared_labels), names_to = "ObjectNumber", values_to = "intensity") %>%
    mutate(ObjectNumber = as.integer(ObjectNumber)) %>%
    pivot_wider(names_from = time, values_from = intensity)

  # Merge and save
  merged_df <- centroids_df %>%
    filter(ObjectNumber %in% intensity_long$ObjectNumber) %>%
    left_join(intensity_long, by = "ObjectNumber") %>%
    mutate(FileID = file_id) %>%
    select(FileID, ObjectNumber, centroid_x, centroid_y, everything())

  out_file <- file.path(ca_results_matched, paste0(file_id, "_matched.csv"))
  write_csv(merged_df, out_file)
}


```

## IIb2. Helper Function – parse through matched df folder and add metadata
```{r}
# -----------------------------
# Helper Function – parse through matched df folder and add metadata
# -----------------------------
# --- Define paths
matched_dir <- file.path(base_path, "01_matched_dfs")
annotated_dir <- file.path(base_path, "02_annotated_dfs")
dir.create(annotated_dir, recursive = TRUE, showWarnings = FALSE)

# Process all matched files
matched_files <- list.files(matched_dir, full.names = TRUE, pattern = "_matched\\.csv$")

for (file in matched_files) {
  df <- readr::read_csv(file, show_col_types = FALSE)
  file_base <- str_remove(basename(file), "_matched\\.csv$")
  parts <- str_split(file_base, "_", simplify = TRUE)

  # Sanity check
  if (ncol(parts) < 6) {
    message("Skipping malformed filename: ", file)
    next
  }

  # Parse metadata
  meta_row <- tibble(
    FileID    = file_base,
    Celltype  = parts[, 1],
    Timepoint = parts[, 2],
    Genotype  = parts[, 3],
    Treatment = paste(parts[, 4], parts[, 5], sep = "_"),
    Replicate = parts[, 6]
  )

  # Avoid column conflicts
  meta_clean <- meta_row %>%
    select(FileID, Celltype, Timepoint, Genotype, Treatment, Replicate) %>%
    select(-any_of(intersect(names(.), names(df))))

  # Add metadata to each row
  annotated_df <- bind_cols(meta_clean[rep(1, nrow(df)), ], df)

  # Save
  out_file <- file.path(annotated_dir, paste0(file_base, "_annotated.csv"))
  readr::write_csv(annotated_df, out_file)
}
```


## IIb3. Helper Function - Merge combined_dfs and compute baseline threshold  
```{r}
# -----------------------------
# Helper Function – Merge annotated CSVs, rescale traces, compute threshold, and apply floor
# -----------------------------

# Define paths
annotated_dir    <- file.path(base_path, "02_annotated_dfs")
thresholded_dir  <- file.path(base_path, "03_thresholded_dfs")
dir.create(thresholded_dir, recursive = TRUE, showWarnings = FALSE)

# Load all annotated CSVs
annotated_files <- list.files(annotated_dir, full.names = TRUE, pattern = "_annotated\\.csv$")
stopifnot(length(annotated_files) > 0)

# Merge
intensity_df <- map_dfr(annotated_files, readr::read_csv, show_col_types = FALSE)

# Optional merged output
write.csv(intensity_df, file = file.path(ca_results_dataframes, "merged_results.csv"), row.names = FALSE)

# -----------------------------
# Rescale helper function (rolling min-max per ROI trace)
rescale_rolling <- function(x, k = 50) {
  roll_min <- zoo::rollapply(x, width = k, FUN = min, fill = NA, align = "center", partial = TRUE)
  roll_max <- zoo::rollapply(x, width = k, FUN = max, fill = NA, align = "center", partial = TRUE)
  rng <- roll_max - roll_min
  rng[rng < 1e-6] <- NA  # prevent divide by near-zero
  rescaled <- (x - roll_min) / rng
  rescaled[is.na(rescaled)] <- 0  # optional: fallback to 0
  return(rescaled)
}

# -----------------------------
# Compute ROI means from rescaled traces
rescaled_df <- intensity_df

# Identify time-frame columns
tf_cols <- grep("^[0-9]+$", colnames(rescaled_df), value = TRUE)

# Rescale each ROI (column-wise)
rescaled_df[tf_cols] <- lapply(rescaled_df[tf_cols], rescale_rolling, k = baseline_window)

# Pivot to long format and compute per-ROI means (after rescaling)
roi_means <- rescaled_df %>%
  pivot_longer(cols = all_of(tf_cols), names_to = "ROI", values_to = "Rescaled_Intensity") %>%
  mutate(ROI = as.character(ROI)) %>%
  group_by(FileID, ROI) %>%
  summarise(ROI_mean = mean(Rescaled_Intensity, na.rm = TRUE), .groups = "drop") %>%
  left_join(intensity_df %>% distinct(FileID, Genotype, Treatment), by = "FileID")

# Signal summary (rescaled)
signal_summary <- roi_means %>%
  group_by(Treatment) %>%
  summarise(
    mean_intensity   = mean(ROI_mean, na.rm = TRUE),
    median_intensity = median(ROI_mean, na.rm = TRUE),
    min              = min(ROI_mean, na.rm = TRUE),
    max              = max(ROI_mean, na.rm = TRUE),
    .groups = "drop"
  )

# Compute threshold from specified basal group
roi_means <- roi_means %>%
  mutate(TreatmentGroup = paste(Genotype, Treatment, sep = "_"))

threshold_basal <- roi_means %>%
  filter(TreatmentGroup == basal_group) %>%
  summarise(threshold_basal = 0.75 * median(ROI_mean, na.rm = TRUE)) %>%
  pull(threshold_basal)

stopifnot(is.numeric(threshold_basal), length(threshold_basal) == 1)

# Attach threshold to each file
filter_table <- roi_means %>%
  distinct(FileID) %>%
  mutate(filter_thresh = threshold_basal)

# Percent ROIs above threshold
roi_activity_summary <- roi_means %>%
  left_join(filter_table, by = "FileID") %>%
  mutate(above_thresh = ROI_mean > filter_thresh) %>%
  group_by(FileID) %>%
  summarise(percent_ROIs_above_thresh = mean(above_thresh, na.rm = TRUE) * 100, .groups = "drop")

# Output summaries
print(signal_summary)
print(filter_table)
print(roi_activity_summary)

# -----------------------------
# Apply threshold floor to original traces and save

for (file_path in annotated_files) {
  df <- readr::read_csv(file_path, show_col_types = FALSE)
  
  tf_cols <- grep("^[0-9]+$", colnames(df), value = TRUE)
  df[tf_cols] <- lapply(df[tf_cols], function(col) pmax(col, threshold_basal))
  
  base_name <- str_replace(tools::file_path_sans_ext(basename(file_path)), "_annotated$", "")
  out_name  <- paste0(base_name, "_thresholded.csv")
  out_path <- file.path(thresholded_dir, out_name)
  write.csv(df, out_path, row.names = FALSE)
}

```


## IIb4. Helper Function - F/F calculation
```{r}
# -----------------------------
# Helper Function – dF/F calculation
# -----------------------------

# Setup paths
thresholded_dir <- file.path(base_path, "03_thresholded_dfs")
dFF_dir         <- file.path(base_path, "04_dFF_dfs")
dir.create(dFF_dir, recursive = TRUE, showWarnings = FALSE)

# -----------------------------
# ΔF/F Calculation Function
# -----------------------------
calculate_dff <- function(trace, window = baseline_window, smoothing_k = NULL, percentile = 0.2) {
  if (!is.null(smoothing_k)) {
    trace <- zoo::rollmean(trace, k = smoothing_k, fill = NA)
  }
  if (length(trace) < window || all(is.na(trace))) return(rep(NA, length(trace)))
  baseline <- zoo::rollapply(trace, width = window,
                             FUN = function(x) quantile(x, probs = percentile, na.rm = TRUE),
                             align = "center", fill = NA, partial = TRUE)
  baseline <- pmax(baseline, 1e-3)
  dff <- (trace - baseline) / baseline
  pmax(dff, 0)
}

# -----------------------------
# Process each thresholded file
# -----------------------------
thresholded_files <- list.files(thresholded_dir, full.names = TRUE, pattern = "_thresholded\\.csv$")
stopifnot(length(thresholded_files) > 0)

for (file_path in thresholded_files) {
  df <- readr::read_csv(file_path, show_col_types = FALSE)

  roi_cols <- colnames(df) %>%
    setdiff(c("FileID", "Celltype", "Timepoint", "Genotype", "Treatment", "Replicate",
              "ObjectNumber", "centroid_x", "centroid_y", "ImageNumber")) %>%
    .[str_detect(., "^[0-9]+$")]

  if (!"ImageNumber" %in% colnames(df)) {
    df <- df %>% mutate(ImageNumber = row_number())
  }

  for (col in roi_cols) {
    df[[paste0("dF_F_", col)]] <- calculate_dff(df[[col]], window = baseline_window, smoothing_k = smoothing_k)
  }

  df_clean <- df[, setdiff(colnames(df), roi_cols)]
  out_name <- str_replace(basename(file_path), "_thresholded\\.csv$", "_dFF.csv")
  write.csv(df_clean, file.path(dFF_dir, out_name), row.names = FALSE)
}

```

## IIb5. Processing Loop per dFF Sample 
```{r}
# -----------------------------
# Define Input Files
# -----------------------------
dff_files <- list.files(dFF_dir, full.names = TRUE, pattern = "_dFF\\.csv$")

# -----------------------------
# Helper Functions
# -----------------------------
zscore_trace <- function(x) {
  mu <- mean(x, na.rm = TRUE)
  sd_x <- sd(x, na.rm = TRUE)
  if (is.na(sd_x) || sd_x < 1e-6) return(rep(0, length(x)))
  (x - mu) / sd_x
}

binarize_peaks <- function(trace, threshold = 1, min_dist = 2) {
  peaks <- pracma::findpeaks(trace, minpeakheight = threshold, minpeakdistance = min_dist)
  binary <- rep(0L, length(trace))
  if (!is.null(peaks)) binary[peaks[, 2]] <- 1L
  return(binary)
}

# -----------------------------
# Initialize Result Lists
# -----------------------------
cor_list     <- list()
summary_list <- list()

# -----------------------------
# Process Each dFF File
# -----------------------------
for (file_path in dff_files) {
  df <- readr::read_csv(file_path, show_col_types = FALSE)
  dff_cols  <- grep("^dF_F_[0-9]+$", names(df), value = TRUE)
  roi_ids   <- gsub("^dF_F_", "", dff_cols)
  meta_cols <- setdiff(names(df), dff_cols)
  clean_id  <- tools::file_path_sans_ext(basename(file_path))

  # Create output folder for this file
  sample_dir <- file.path(dFF_dir, clean_id)
  dir.create(sample_dir, recursive = TRUE, showWarnings = FALSE)

  # --- 1. Binarize
  binary_df <- df[, meta_cols, drop = FALSE]
  for (col in dff_cols) {
    trace_z <- zscore_trace(df[[col]])
    binary_df[[col]] <- binarize_peaks(trace_z, threshold = 1, min_dist = 2)
  }
  write.csv(binary_df, file.path(sample_dir, paste0(clean_id, "_binary.csv")), row.names = FALSE)

  # --- 2. Firing Rate
  firing_rate <- sapply(binary_df[dff_cols], function(x) sum(x, na.rm = TRUE) / length(x))
  firing_df <- data.frame(ROI = roi_ids, FiringRate = firing_rate)
  write.csv(firing_df, file.path(sample_dir, paste0(clean_id, "_firingRate.csv")), row.names = FALSE)
  summary_list[[clean_id]] <- firing_df

  # --- 3. Correlation Matrix
  mat_dff <- as.matrix(df[dff_cols])
  valid_rois <- apply(mat_dff, 1, function(x) sd(x, na.rm = TRUE)) > 1e-3
  cor_mat_dff <- cor(t(mat_dff[valid_rois, ]), use = "pairwise.complete.obs")
  cor_mat_dff[!is.finite(cor_mat_dff)] <- 0
  pdf(file.path(sample_dir, paste0(clean_id, "_CorrelationMatrix_dFF.pdf")), width = 6, height = 6)
  pheatmap(cor_mat_dff, clustering_method = "complete", main = "dFF Correlation Matrix", fontsize = 6, border_color = NA)
  dev.off()
  cor_list[[clean_id]] <- cor_mat_dff

  # --- 4. Trace Plots (first 10 ROIs)
  trace_roi_idx <- head(roi_ids, 10)
  for (roi in trace_roi_idx) {
    col_dff <- paste0("dF_F_", roi)
    if (!(col_dff %in% names(df)) || !(col_dff %in% names(binary_df))) {
      message("Skipping ROI ", roi, " - column(s) missing")
      next
    }

    trace_len <- min(length(df[[col_dff]]), length(binary_df[[col_dff]]))
    df_trace <- data.frame(
      Frame  = seq_len(trace_len),
      dFF    = df[[col_dff]][1:trace_len],
      Binary = binary_df[[col_dff]][1:trace_len]
    )

    p_dff <- ggplot(df_trace, aes(x = Frame, y = dFF)) +
      geom_line() + ggtitle("ΔF/F") + theme_minimal()
    p_bin <- ggplot(df_trace, aes(x = Frame, y = Binary)) +
      geom_step(color = "firebrick2") + ggtitle("Binary") + theme_minimal()

    ggsave(file.path(sample_dir, paste0("ROI_", roi, "_trace.pdf")),
           p_dff / p_bin, width = 6, height = 5, device = cairo_pdf)
  }
}

# -----------------------------
# Save Global Summary Tables
# -----------------------------
all_summary <- bind_rows(summary_list, .id = "SampleID")
write.csv(all_summary, file.path(dFF_dir, "summary_firing_rates_all.csv"), row.names = FALSE)

all_cor_flat <- lapply(cor_list, function(mat) {
  if (is.null(rownames(mat))) return(NULL)
  cor_df <- as.data.frame(as.table(mat))
  names(cor_df) <- c("ROI1", "ROI2", "Correlation")
  return(cor_df)
})
all_cor_flat <- bind_rows(all_cor_flat, .id = "SampleID")
write.csv(all_cor_flat, file.path(dFF_dir, "summary_correlations_all.csv"), row.names = FALSE)
```




## ???IId. Trace Plotting (per sample), Correlations, Network plots and Firing Statistics 
```{r}
# -------------------------------------------------
# Trace Plotting (per sample), Correlations, Network plots and Firing Statistics 
# -------------------------------------------------

# --- Setup
trace_dir <- file.path(base_path, "label_traces")
dir.create(trace_dir, recursive = TRUE, showWarnings = FALSE)

all_sample_ids <- unique(dff_results$FileID)
centroid_df_clean <- dff_results %>%
  distinct(FileID, ObjectNumber, centroid_x, centroid_y)

# --- Palettes
corr_palette <- colorRampPalette(rev(RColorBrewer::brewer.pal(11, "RdYlBu")))(100)
node_palette <- colorRampPalette(rev(RColorBrewer::brewer.pal(9, "YlGnBu")))(100)

for (sample_id in all_sample_ids) {
  clean_sample_id <- gsub("_centroids_valid.csv$", "", basename(sample_id))
  output_dir <- file.path(base_path, clean_sample_id)
  dir.create(output_dir, recursive = TRUE, showWarnings = FALSE)

  # --- Retrieve scalar threshold
  bin_thresh_scalar <- filter_table %>%
    filter(FileID == sample_id) %>%
    pull(filter_thresh)

  if (length(bin_thresh_scalar) != 1 || is.na(bin_thresh_scalar)) {
    warning(paste("Skipping", sample_id, "- missing or ambiguous threshold"))
    next
  }

  valid_rois <- dff_results %>%
    filter(FileID == sample_id) %>%
    group_by(ObjectNumber) %>%
    summarise(roi_sd = sd(dF_F, na.rm = TRUE)) %>%
    filter(roi_sd > 0)
  
  df_sample <- dff_results %>%
    filter(FileID == sample_id) %>%
    inner_join(valid_rois, by = "ObjectNumber") %>%
    arrange(ObjectNumber, ImageNumber) %>%
    group_by(ObjectNumber) %>%
    mutate(
      dF_F_scaled = dF_F / max(dF_F, na.rm = TRUE),
      bin_thresh  = 0.5 * bin_thresh_scalar,
      Treatment   = first(Treatment),
      dF_F_binary = ifelse(Treatment == inh_ref, 0, ifelse(dF_F > bin_thresh, 1, 0)),
      spike_amplitude = ifelse(dF_F_binary == 1, dF_F_scaled, NA)
    ) %>%
    ungroup()
  
  # --- ROI trace plots
  roi_ids <- unique(df_sample$ObjectNumber)
  for (i in seq_along(roi_ids)) {
    df_trace <- dplyr::filter(df_sample, ObjectNumber == roi_ids[i])
    p_raw    <- ggplot(df_trace, aes(x = ImageNumber, y = Intensity_IntegratedIntensity_Fura)) + geom_line() + theme_minimal()
    p_scaled <- ggplot(df_trace, aes(x = ImageNumber, y = dF_F_scaled)) + geom_line(color = "dodgerblue") + theme_minimal()
    p_binary <- ggplot(df_trace, aes(x = ImageNumber, y = dF_F_binary)) + geom_step(color = "firebrick2") + theme_minimal()
    ggsave(file.path(trace_dir, paste0(clean_sample_id, "_ROI_", i, ".pdf")),
           p_raw / p_scaled / p_binary, width = 6, height = 6, device = cairo_pdf)
  }

  # --- Binary matrix and correlation matrix
  mat <- df_sample %>%
    dplyr::select(ObjectNumber, ImageNumber, dF_F_binary) %>%
    tidyr::pivot_wider(names_from = ImageNumber, values_from = dF_F_binary) %>%
    dplyr::arrange(ObjectNumber)
  mat_matrix <- as.matrix(dplyr::select(mat, -ObjectNumber))
  rownames(mat_matrix) <- mat$ObjectNumber
  cor_mat <- cor(t(mat_matrix), use = "pairwise.complete.obs")
  cor_mat[!is.finite(cor_mat)] <- 0
  
  valid_corr_vals <- cor_mat[upper.tri(cor_mat) & is.finite(cor_mat)]
  min_corr <- min(valid_corr_vals, na.rm = TRUE)
  max_corr <- max(valid_corr_vals, na.rm = TRUE)
  
  # Handle case where all values are equal or NA
  if (length(valid_corr_vals) == 0 || min_corr == max_corr || !is.finite(min_corr) || !is.finite(max_corr)) {
    warning(paste("Skipping", sample_id, "- correlation matrix is flat or invalid"))
    next
  }
    
  pdf(file.path(output_dir, paste0(clean_sample_id, "_CorrelationMatrix.pdf")), width = 6, height = 6)
  pheatmap(cor_mat, clustering_method = "complete", main = "Correlation Matrix", border_color = NA, fontsize = 6)
  dev.off()

  # --- Identify synchronized ROIs based on roi_corr_threshold
  roi_edge_idx <- which(cor_mat > roi_corr_threshold & upper.tri(cor_mat), arr.ind = TRUE)
  roi_high_corr_ids <- unique(c(rownames(cor_mat)[roi_edge_idx[, 1]], rownames(cor_mat)[roi_edge_idx[, 2]]))

  # --- Group label from map
  group_label <- if (sample_id %in% names(genotype_map)) genotype_map[[sample_id]] else "Unknown"

  # --- Filter short/long bursts
  df_sample <- df_sample %>%
    dplyr::group_by(ObjectNumber) %>%
    dplyr::arrange(ImageNumber, .by_group = TRUE) %>%
    dplyr::mutate(
      run_id   = data.table::rleid(dF_F_binary),
      run_len  = ave(dF_F_binary, ObjectNumber, run_id, FUN = length),
      dF_F_binary = ifelse(dF_F_binary == 1 & (run_len < 1 | run_len > 60), 0, dF_F_binary)
    ) %>%
    dplyr::ungroup()

  # --- Recompute rising edge
  df_sample <- df_sample %>%
    dplyr::group_by(ObjectNumber) %>%
    dplyr::arrange(ImageNumber, .by_group = TRUE) %>%
    dplyr::mutate(rising_edge = dF_F_binary == 1 & lag(dF_F_binary, default = 0) == 0) %>%
    dplyr::ungroup()

  # --- Tag synchronized ROIs
  df_sample <- df_sample %>%
    dplyr::mutate(is_high_corr = as.character(ObjectNumber) %in% roi_high_corr_ids)

  # --- Firing stats (by ROI)
  firing_stats <- df_sample %>%
    dplyr::group_by(ObjectNumber, is_high_corr) %>%
    dplyr::summarise(
      firing_events   = sum(rising_edge, na.rm = TRUE),
      total_frames    = dplyr::n(),
      active_frames   = sum(dF_F_binary, na.rm = TRUE),
      percent_active  = 100 * active_frames / total_frames,
      spike_amplitude = if (sum(dF_F_binary == 1 & dF_F_scaled > 0.2, na.rm = TRUE) > 0) {
        quantile(dF_F_scaled[dF_F_binary == 1 & dF_F_scaled > 0.2], 0.75, na.rm = TRUE)
      } else {
        NA_real_
      },
      .groups = "drop"
    ) %>%
    dplyr::mutate(
      Sample = sample_id,
      firing_rate_per_min = firing_events / (max(df_sample$ImageNumber, na.rm = TRUE) / (frame_rate * 60)),
      Group = group_label
    )
  events_list[[clean_sample_id]] <- firing_stats

  # --- Sample-level summary
  n_cells        <- dplyr::n_distinct(df_sample$ObjectNumber)
  n_frames_total <- max(df_sample$ImageNumber, na.rm = TRUE)
  duration_min   <- n_frames_total / (frame_rate * 60)

  sample_summary <- firing_stats %>%
    dplyr::summarise(
      Sample                          = clean_sample_id,
      Total_cells                     = n_cells,
      Total_firing_events             = sum(firing_events, na.rm = TRUE),
      Firing_events_per_min           = Total_firing_events / duration_min,
      Firing_events_per_min_norm      = Firing_events_per_min / n_cells,
      Cells_active                    = sum(firing_events > 0, na.rm = TRUE),
      Percent_cells_active            = 100 * Cells_active / n_cells,
      Mean_percent_active_frames      = mean(percent_active, na.rm = TRUE),
      Mean_percent_active_frames_norm = Mean_percent_active_frames
    )
  summary_list[[clean_sample_id]] <- sample_summary

  # --- Correlation-based network edges
  edge_list <- which(cor_mat > correlation_threshold & upper.tri(cor_mat), arr.ind = TRUE)
  edges <- data.frame(
    from = rownames(cor_mat)[edge_list[, 1]],
    to   = rownames(cor_mat)[edge_list[, 2]],
    corr = cor_mat[edge_list]
  )

  valid_corr_vals <- cor_mat[upper.tri(cor_mat) & is.finite(cor_mat)]
  min_corr <- min(valid_corr_vals, na.rm = TRUE)
  max_corr <- max(valid_corr_vals, na.rm = TRUE)
  
  # Ensure min ≠ max to avoid non-unique breaks
  if (min_corr == max_corr) {
    min_corr <- min_corr - 0.01
    max_corr <- max_corr + 0.01
  }
  
  if (nrow(edges) == 0 || min_corr == max_corr || !is.finite(min_corr) || !is.finite(max_corr)) {
    # fallback: use fixed color for all edges
    edges$corr_scaled <- 50
    edges$color <- rep("grey70", nrow(edges))
  } else {
    edges$corr_scaled <- scales::rescale(edges$corr, to = c(1, 100), from = c(min_corr, max_corr))
    edges$corr_scaled <- pmin(pmax(round(edges$corr_scaled), 1), 100)
    edges$color <- corr_palette[edges$corr_scaled]
  }
  
  write.csv(edges, file.path(output_dir, "edge_list_with_colors.csv"), row.names = FALSE)

  # --- Coordinates and node coloring
  coords <- df_sample %>%
    dplyr::filter(!is.na(centroid_x), !is.na(centroid_y)) %>%
    dplyr::group_by(ObjectNumber) %>%
    dplyr::slice(1) %>%
    dplyr::ungroup() %>%
    dplyr::distinct(ObjectNumber, centroid_x, centroid_y) %>%
    dplyr::mutate(Label = as.character(ObjectNumber))

  spike_rates <- df_sample %>%
    dplyr::group_by(ObjectNumber) %>%
    dplyr::summarise(spike_rate = mean(dF_F_binary, na.rm = TRUE)) %>%
    dplyr::mutate(Label = as.character(ObjectNumber))

  coords <- dplyr::left_join(coords, spike_rates, by = "Label") %>%
    dplyr::mutate(
      color_idx = pmin(pmax(round(scales::rescale(spike_rate, to = c(1, 100))), 1), 100),
      color = node_palette[color_idx]
    )

  # --- Network plotting wrapper
  plot_network <- function(node_colors, edge_colors, legend_title, legend_range, file_name) {
    pdf(file.path(output_dir, file_name), width = 6.9, height = 5)
    layout(matrix(c(1, 2), nrow = 1), widths = c(5, 1.5))
    par(mar = c(4, 4, 2, 1))
    plot(coords$centroid_x, coords$centroid_y,
         pch = 21, bg = node_colors, col = "grey50",
         xlab = "X", ylab = "Y", main = clean_sample_id,
         asp = 1, xlim = c(0, image_dim[1]), ylim = c(0, image_dim[2]))
    for (i in seq_len(nrow(edges))) {
      p1 <- dplyr::filter(coords, Label == edges$from[i])
      p2 <- dplyr::filter(coords, Label == edges$to[i])
      if (nrow(p1) == 1 && nrow(p2) == 1) {
        segments(p1$centroid_x, p1$centroid_y, p2$centroid_x, p2$centroid_y,
                 col = edge_colors[i], lwd = 1)
      }
    }
    par(mar = c(4, 2, 2, 4))
    legend_img <- as.raster(matrix(rev(corr_palette), ncol = 1))
    plot(NA, xlim = c(0, 1), ylim = legend_range, type = "n", axes = FALSE, xlab = "", ylab = "")
    rasterImage(legend_img, 0, legend_range[1], 1, legend_range[2])
    axis(4, at = pretty(legend_range), las = 1)
    mtext(legend_title, side = 4, line = 2.5)
    dev.off()
  }

  # --- Network visualizations
  plot_network(rep("grey50", nrow(coords)), edges$color, "Correlation", c(min_corr, max_corr), "network_from_centroids.pdf")
  plot_network(coords$color, edges$color, "Spike rate", c(min_corr, max_corr), "network_from_centroids_spikerate.pdf")

  # --- Save processed data
  write.csv(filter(firing_stats, is_high_corr == TRUE),  file.path(output_dir, paste0(clean_sample_id, "_firing_stats_highcorr.csv")), row.names = FALSE)
  write.csv(filter(firing_stats, is_high_corr == FALSE), file.path(output_dir, paste0(clean_sample_id, "_firing_stats_lowcorr.csv")),  row.names = FALSE)
  write.csv(df_sample, file.path(output_dir, paste0(clean_sample_id, "_df_processed.csv")), row.names = FALSE)

  # --- Save correlation matrix (long)
  cor_long <- as.data.frame(as.table(cor_mat)) %>%
    dplyr::rename(from = Var1, to = Var2, corr = Freq) %>%
    dplyr::mutate(Sample = sample_id) %>%
    dplyr::filter(from != to)
  cor_list[[clean_sample_id]] <- cor_long

  # --- Louvain community detection
  g <- igraph::graph_from_data_frame(edges, directed = FALSE)
  if (igraph::gorder(g) > 0 && igraph::gsize(g) > 0) {
    comms <- igraph::cluster_louvain(g)
    membership <- membership(comms)
    community_sizes <- sizes(comms)

    membership_df <- data.frame(
      ObjectNumber   = names(membership),
      Community      = membership,
      Sample         = sample_id,
      CommunitySize  = as.integer(community_sizes[as.character(membership)])
    )
    comm_list[[clean_sample_id]] <- membership_df
  }
}




#dev.off()


```





# III.  Plotting & Statistical Evaluation
## IIIa. ROI Couting and Event Rates Calculation 
```{r}
## ---------------------------------------------------------------------------
## Plot CellposeSAM / CNER results — incl. events per min⁻¹ (±SEM)
## ---------------------------------------------------------------------------
# Aggregate sample-level summaries
normalised_metrics <- bind_with_sample(summary_list)
# --------------------------
# Helpers
# --------------------------

# Combine list entries into one dataframe and append sample name
bind_with_sample <- function(lst) {
  imap_dfr(lst, ~ {
    if (!"Sample" %in% names(.x))
      .x <- mutate(.x, Sample = .y)
    .x
  })
}

# Standardize sample names for matching
clean_sample_names <- function(s) {
  s %>%
    str_replace("_centroids_valid\\.csv$", "") %>%
    str_replace("_\\d+$", "")
}

# --------------------------
# Bind event and community data
# --------------------------
events_df <- bind_with_sample(events_list)
comm_df   <- bind_with_sample(comm_list)

comm_df <- comm_df %>% mutate(Label = as.character(ObjectNumber))

stopifnot("Sample" %in% names(events_df), "Sample" %in% names(comm_df))

# --------------------------
# Standardize Sample & Assign Genotype
# --------------------------

events_df <- events_df %>%
  mutate(
    Sample   = clean_sample_names(as.character(Sample)),
    Label    = as.character(ObjectNumber),
    Genotype = genotype_map[Sample]
  )

comm_df <- comm_df %>%
  mutate(
    Sample   = clean_sample_names(as.character(Sample)),
    Label    = as.character(ObjectNumber),
    Genotype = genotype_map[Sample]
  )

normalised_metrics <- normalised_metrics %>%
  mutate(
    Sample   = clean_sample_names(as.character(Sample)),
    Genotype = genotype_map[Sample]
  )

# --------------------------
# ROI count and events/min calculation
# --------------------------

events_df <- events_df %>%
  group_by(Sample) %>%
  mutate(
    n_labels            = n(),
    events_per_min      = firing_events / (total_frames / (frame_rate * 60)),
    events_per_min_norm = events_per_min / n_labels
  ) %>%
  ungroup()

# --------------------------
# Propagate is_high_corr to sample-level
# --------------------------

events_corr <- events_df %>%
  select(Sample, is_high_corr) %>%
  distinct()

normalised_metrics <- normalised_metrics %>%
  left_join(events_corr, by = "Sample")

# Ensure matching types
events_df <- events_df %>%
  mutate(ObjectNumber = as.character(ObjectNumber))

comm_df <- comm_df %>%
  mutate(ObjectNumber = as.character(ObjectNumber))

# Add is_high_corr to comm_df by (Sample, ObjectNumber)
comm_df <- comm_df %>%
  left_join(events_df %>% select(Sample, ObjectNumber, is_high_corr),
            by = c("Sample", "ObjectNumber"))


# --------------------------
# Assign Genotype Colors
# --------------------------

unique_genos <- unique(na.omit(c(events_df$Genotype, comm_df$Genotype)))
geno_cols <- setNames(colorRampPalette(base_colors)(length(unique_genos)), unique_genos)

```


## IIIb. Violin plots with Kruskal-Wallis test 
```{r}
# ----------------------------------------------------------
# Helper: Violin plot with significance stats
# ----------------------------------------------------------
plot_genotype_violin_with_stats <- function(df, metric, ylab, output_prefix, base_dir = ca_results_dir,
                                            add_sem = FALSE, jitter_color = "grey50") {
  require(ggpubr)

  metric_sym <- rlang::sym(metric)

  # # Ensure correct genotype order
  # Consistent genotype label ordering from sample_keys
  df <- df %>%
    mutate(Genotype = factor(Genotype, levels = names(geno_cols)))

  # Add early exit if only one genotype
  if (length(unique(na.omit(df$Genotype))) < 2) {
    message("Skipping plot: Only one genotype present in the data subset.")
    return(NULL)
  }
  
  # Kruskal-Wallis
  kruskal_pval <- kruskal.test(reformulate("Genotype", metric), data = df)$p.value

  # Pairwise Wilcoxon
  pw <- pairwise.wilcox.test(df[[metric]], df$Genotype, p.adjust.method = "holm")
  pairwise_df <- as.data.frame(as.table(pw$p.value), stringsAsFactors = FALSE) %>%
    dplyr::rename(group1 = Var1, group2 = Var2, p.value = Freq) %>%
    dplyr::filter(!is.na(p.value)) %>%
    dplyr::mutate(
      significance = case_when(
        p.value < 0.001 ~ "***",
        p.value < 0.01  ~ "**",
        p.value < 0.05  ~ "*",
        TRUE            ~ NA_character_
      ),
      y.position = seq(from = max(df[[metric]], na.rm = TRUE) * 1.05,
                       by = max(df[[metric]], na.rm = TRUE) * 0.05,
                       length.out = dplyr::n())
    ) %>%
    filter(!is.na(significance))

  # Mean ± SEM if requested
  if (add_sem) {
    mean_df <- df %>%
      group_by(Genotype) %>%
      summarise(
        mean_val = mean(!!metric_sym, na.rm = TRUE),
        sem      = sd(!!metric_sym, na.rm = TRUE) / sqrt(n()),
        .groups = "drop"
      )
  }

  # Plot
  p <- ggplot(df, aes(x = Genotype, y = !!metric_sym, fill = Genotype)) +
    geom_violin(scale = "width", trim = FALSE, color = NA, width = 0.85) +
    geom_jitter(width = 0.1, size = 0.5, alpha = 0.5, color = jitter_color) +
    geom_boxplot(width = 0.2, fill = "white", outlier.shape = NA, linewidth = 0.3) +
    scale_fill_manual(values = geno_cols) +
    labs(y = ylab, x = NULL) +
    theme_bw(base_size = 6) +
    theme(panel.grid = element_blank(), legend.position = "none", aspect.ratio = 1.5,axis.text.x = element_text(angle = 90, hjust = 0.5))

  if (nrow(pairwise_df) > 0) {
    p <- p + stat_pvalue_manual(pairwise_df,
                                label = "significance",
                                y.position = "y.position",
                                xmin = "group1", xmax = "group2",
                                tip.length = 0.01, size = 2)
  }

  ggsave(file.path(base_dir, paste0(output_prefix, "_violin.pdf")),
         p, width = 3, height = 5, device = cairo_pdf)

  # Return
  kruskal_df <- tibble(
    group1 = "Kruskal-Wallis", group2 = "",
    p.value = kruskal_pval,
    significance = case_when(
      kruskal_pval < 0.001 ~ "***",
      kruskal_pval < 0.01  ~ "**",
      kruskal_pval < 0.05  ~ "*",
      TRUE                 ~ "ns"
    )
  )

  return(list(p = p, stats = bind_rows(kruskal_df, pairwise_df)))
}
```


## IIIc. pairwise Wilcoxon tests
```{r}
# ----------------------------------------------------------
# Run comparisons
# ----------------------------------------------------------

# Split events_df by ROI correlation
events_high <- events_df %>% filter(is_high_corr == TRUE)
events_low  <- events_df %>% filter(is_high_corr == FALSE)

# Per-cell metrics
res_high      <- plot_genotype_violin_with_stats(events_high, "firing_events",   "Firing Events",    "firing_highcorr")
res_low       <- plot_genotype_violin_with_stats(events_low,  "firing_events",   "Firing Events",    "firing_lowcorr")
res_high_amp  <- plot_genotype_violin_with_stats(events_high, "spike_amplitude", "Spike Amplitude",  "amp_highcorr")
res_low_amp   <- plot_genotype_violin_with_stats(events_low,  "spike_amplitude", "Spike Amplitude",  "amp_lowcorr")

# Sample-level normalized
res_fire_norm <- plot_genotype_violin_with_stats(normalised_metrics, "Firing_events_per_min_norm",
                                                 "Events · min⁻¹ / ROI", "firing_norm")
res_active_norm <- plot_genotype_violin_with_stats(normalised_metrics, "Mean_percent_active_frames_norm",
                                                   "% Active / ROI", "active_norm")
```


## IIId. Save combined Outputs & Sample-Level Normalized Outputs
```{r}
# ------------------------------------------------------
# Save Combined Outputs (Cell-Pair Correlations, Communities, Events)
# ------------------------------------------------------
write.csv(bind_rows(cor_list),    file.path(ca_results_dataframes, "combined_cell_pairwise_correlations.csv"), row.names = FALSE)
write.csv(bind_rows(comm_list),   file.path(ca_results_dataframes, "combined_cell_community_membership.csv"),  row.names = FALSE)
write.csv(bind_rows(events_list), file.path(ca_results_dataframes, "combined_events_per_min_results.csv"),     row.names = FALSE)

# ------------------------------------------------------
# Save sample-level normalized metrics (already annotated with genotype)
# ------------------------------------------------------
write.csv(normalised_metrics,
          file.path(ca_results_dataframes, "combined_sample_level_normalised_metrics.csv"),
          row.names = FALSE)

```


## IIIe. Sample-Level Normalized Metrics
```{r}
# ---------------------------------------------------------------------------
# Sample‑level normalised metrics from `normalised_metrics`
# ---------------------------------------------------------------------------

# plot low / high corr filtered violin metrics
# firing rate 
p_fire_norm_result <- plot_genotype_violin_with_stats(
  normalised_metrics,
  metric = "Firing_events_per_min_norm",
  ylab = "Events · min⁻¹ / ROI",
  output_prefix = "fire_events_norm",
  base_dir = ca_results_dir
)
p_active_norm_result <- plot_genotype_violin_with_stats(
  normalised_metrics,
  metric = "Mean_percent_active_frames_norm",
  ylab = "% Active / ROI",
  output_prefix = "percent_active_norm",
  base_dir = ca_results_dir
)


# plot low / high corr filtered firing rates 
p_fire_norm_high_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == TRUE),
  metric = "events_per_min_norm",
  ylab = "Events · min⁻¹ / ROI",
  output_prefix = "fire_events_norm_high",
  base_dir = ca_results_dir
)
p_fire_norm_low_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == FALSE),
  metric = "events_per_min_norm",
  ylab = "Events · min⁻¹ / ROI",
  output_prefix = "fire_events_norm_low",
  base_dir = ca_results_dir
)


# plot low / high active frames
p_active_norm_high_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == TRUE),
  metric = "percent_active",
  ylab = "% Active / ROI",
  output_prefix = "percent_active_norm_high",
  base_dir = ca_results_dir
)
p_active_norm_low_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == FALSE),
  metric = "percent_active",
  ylab = "% Active / ROI",
  output_prefix = "percent_active_norm_low",
  base_dir = ca_results_dir
)

# plot low / high spike amplitude  
p_amp_orig_high_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == TRUE),
  metric = "spike_amplitude",
  ylab = "Spike Amplitude",
  output_prefix = "spike_amplitude_high",
  base_dir = ca_results_dir
)
p_amp_orig_low_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == FALSE),
  metric = "spike_amplitude",
  ylab = "Spike Amplitude",
  output_prefix = "spike_amplitude_low",
  base_dir = ca_results_dir
)


# Extract violin plots
p_fire_norm         <- p_fire_norm_result$p
p_active_norm       <- p_active_norm_result$p

p_fire_norm_high    <- p_fire_norm_high_results$p
p_fire_norm_low     <- p_fire_norm_low_results$p

p_active_norm_high  <- p_active_norm_high_results$p
p_active_norm_low   <- p_active_norm_low_results$p

p_amp_orig_high     <- p_amp_orig_high_results$p
p_amp_orig_low      <- p_amp_orig_low_results$p

```



## IIIf. Community Size Analysis (Non-parametric) 
```{r}
# ----------------------------------------------------------------------
# Stratify by high/low correlation
# ----------------------------------------------------------------------
comm_high <- comm_df %>% filter(is_high_corr == TRUE)
comm_low  <- comm_df %>% filter(is_high_corr == FALSE)

# ----------------------------------------------------------------------
# Helper to run KW + Wilcoxon + plot
# ----------------------------------------------------------------------
analyze_comm_size <- function(df, label, outfile_prefix) {
  df <- df %>% filter(!is.na(Genotype)) # drop NA genotypes
  df$Genotype <- factor(df$Genotype)    # convert to factor with clean levels
  # Skip if only one genotype
  if (length(unique(df$Genotype[!is.na(df$Genotype)])) < 2) {
    message("Skipping analysis: only one genotype present.")
    return(list(plot = NULL, stats = tibble()))
  }

  # Kruskal-Wallis test
  kw_pval <- kruskal.test(CommunitySize ~ Genotype, data = df)$p.value

  # Pairwise Wilcoxon
  pw <- pairwise.wilcox.test(df$CommunitySize, df$Genotype, p.adjust.method = "holm")
  pairwise_df <- as.data.frame(as.table(pw$p.value), stringsAsFactors = FALSE) %>%
    rename(group1 = Var1, group2 = Var2, p.value = Freq) %>%
    filter(!is.na(p.value)) %>%
    mutate(
      significance = case_when(
        p.value < 0.001 ~ "***",
        p.value < 0.01  ~ "**",
        p.value < 0.05  ~ "*",
        TRUE            ~ "ns"
      ),
      y.position = seq(from = max(df$CommunitySize, na.rm = TRUE) * 1.05,
                       by = max(df$CommunitySize, na.rm = TRUE) * 0.05,
                       length.out = dplyr::n()),
      group1 = factor(group1, levels = levels(df$Genotype)),
      group2 = factor(group2, levels = levels(df$Genotype))
    )

  # Plot
  p <- ggplot(df, aes(x = Genotype, y = CommunitySize, fill = Genotype)) +
    geom_jitter(color = "grey80", width = 0.1, size = 0.5, alpha = 0.5) +
    geom_violin(scale = "width", trim = FALSE, color = NA, width = 0.85, adjust = 1.5) +
    geom_boxplot(width = 0.2, fill = "white", outlier.shape = NA, color = "black", linewidth = 0.3) +
    scale_fill_manual(values = geno_cols) +
    labs(y = "Community Size", x = NULL, title = label) +
    theme_bw(base_size = 6) +
    theme(legend.position = "none",
          aspect.ratio    = 2,
          panel.grid = element_blank(),
          axis.text.x = element_text(angle = 90, hjust = 0.5))

  if (nrow(pairwise_df) > 0) {
    p <- p + stat_pvalue_manual(pairwise_df,
                                label = "significance",
                                y.position = "y.position",
                                xmin = "group1", xmax = "group2",
                                tip.length = 0.01, size = 2)
  }

  ggsave(file.path(ca_results_dir, paste0(outfile_prefix, ".pdf")),
         p, width = 2, height = 3, device = cairo_pdf)

  return(list(p = p,
              stats = bind_rows(
                tibble(group1 = "Kruskal-Wallis", group2 = "", p.value = kw_pval,
                       significance = case_when(
                         kw_pval < 0.001 ~ "***",
                         kw_pval < 0.01  ~ "**",
                         kw_pval < 0.05  ~ "*",
                         TRUE            ~ "ns")),
                pairwise_df)))
}

# ----------------------------------------------------------------------
# Run per-group comparisons
# ----------------------------------------------------------------------
res_comm_high <- analyze_comm_size(comm_high, "Community Size (High Corr ROIs)", "community_size_high")
res_comm_low  <- analyze_comm_size(comm_low,  "Community Size (Low Corr ROIs)",  "community_size_low")

# ----------------------------------------------------------------------
# Save combined stats
# ----------------------------------------------------------------------
stats_comm_all <- bind_rows(
  res_comm_high$stats %>% mutate(Source = "high_corr_ROIs"),
  res_comm_low$stats  %>% mutate(Source = "low_corr_ROIs")
)

write.csv(stats_comm_all,
          file = file.path(ca_results_dataframes, "community_size_stats_high_vs_low.csv"),
          row.names = FALSE)



# plots
p_comm_high <- plot_genotype_violin_with_stats(
  df = comm_high,
  metric = "CommunitySize",
  ylab = "# Communities",
  output_prefix = "community_stats_high",
  base_dir = ca_results_dir
)

p_comm_low <- plot_genotype_violin_with_stats(
  df = comm_low,
  metric = "CommunitySize",
  ylab = "# Communities",
  output_prefix = "community_stats_low",
  base_dir = ca_results_dir
)
```



## IIIg. Time-binned Firing Metrics Over Time
```{r}
# -------------------------------------------------
# Time-binned Firing Metrics Over Time (High vs Low Corr)
# -------------------------------------------------
# Ensure character type for join keys
centroids_df <- centroids_df %>%
  mutate(
    ObjectNumber = as.character(ObjectNumber),
    SampleID_trimmed = str_remove(FileID, "_\\d{3}$")
  )

# Get is_high_corr from events_df
events_corr <- events_df %>%
  select(Sample, ObjectNumber, is_high_corr) %>%
  mutate(ObjectNumber = as.character(ObjectNumber))

# Join is_high_corr into centroids_df
centroids_df <- centroids_df %>%
  left_join(events_corr, by = c("SampleID_trimmed" = "Sample", "ObjectNumber"))

# Ensure ObjectNumber is character in dff_results
dff_results <- dff_results %>%
  mutate(ObjectNumber = as.character(ObjectNumber))

# Add genotype, bin, and join metadata
traces_with_genotype <- dff_results %>%
  left_join(centroids_df, by = c("FileID", "ObjectNumber")) %>%
  mutate(
    Bin = floor(ImageNumber / baseline_window),
    Genotype = purrr::map_chr(FileID, function(s) {
      matched <- names(genotype_map)[str_detect(s, fixed(names(genotype_map)))]
      if (length(matched) > 0) genotype_map[[matched[1]]] else NA_character_
    })
  ) %>%
  filter(!is.na(Genotype), !is.na(is_high_corr))

# Calculate metrics per cell per bin
binned_metrics <- traces_with_genotype %>%
  group_by(FileID, ObjectNumber, Bin, Genotype, is_high_corr) %>%
  arrange(ImageNumber, .by_group = TRUE) %>%
  mutate(
    dF_F = calculate_dff(Intensity_IntegratedIntensity_Fura,
                         window = baseline_window,
                         smoothing_k = smoothing_k,
                         percentile = 0.2),
    dF_F_scaled = if (all(is.na(dF_F))) 0 else dF_F / max(dF_F, na.rm = TRUE),
    dF_F_binary = ifelse(dF_F > threshold_factor * sd(dF_F, na.rm = TRUE), 1, 0),
    spike_amplitude = ifelse(dF_F_binary == 1, dF_F_scaled, NA_real_)
  ) %>%
  mutate(rising_edge = dF_F_binary == 1 & lag(dF_F_binary, default = 0) == 0) %>%
  summarise(
    firing_events = sum(rising_edge, na.rm = TRUE),
    total_frames = n(),
    percent_active = 100 * sum(dF_F_binary, na.rm = TRUE) / total_frames,
    spike_amplitude = mean(spike_amplitude, na.rm = TRUE),
    .groups = "drop"
  )

# Summarize per genotype, bin, and corr group
plot_df_split <- binned_metrics %>%
  group_by(Genotype, Bin, is_high_corr) %>%
  summarise(
    mean_events = mean(firing_events, na.rm = TRUE),
    sem_events  = sd(firing_events, na.rm = TRUE) / sqrt(n()),
    mean_active = mean(percent_active, na.rm = TRUE),
    sem_active  = sd(percent_active, na.rm = TRUE) / sqrt(n()),
    mean_amp    = mean(spike_amplitude, na.rm = TRUE),
    sem_amp     = sd(spike_amplitude, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"
  ) %>%
  mutate(Genotype = factor(Genotype, levels = names(geno_cols)))

# Plot helper
plot_timecourse <- function(data, metric, sem, ylab) {
  data <- data %>%
    mutate(Frame = Bin * baseline_window)

  ggplot(data, aes(x = Frame, y = .data[[metric]], color = Genotype)) +
    geom_line(linewidth = 0.5) +
    geom_ribbon(aes(ymin = .data[[metric]] - .data[[sem]],
                    ymax = .data[[metric]] + .data[[sem]],
                    fill = Genotype), alpha = 0.3, color = NA) +
    labs(x = "Frame", y = ylab) +
    scale_color_manual(values = geno_cols) +
    scale_fill_manual(values = geno_cols) +
    theme_bw(base_size = 6) +
    theme(panel.grid = element_blank(),
          axis.text.x = element_text(angle = 90, hjust = 0.5),
          legend.position = "right")
}
# Generate and save plots by correlation group
for (corr_state in c(TRUE, FALSE)) {
  tag <- if (corr_state) "highcorr" else "lowcorr"
  df_sub <- filter(plot_df_split, is_high_corr == corr_state)

  p1 <- plot_timecourse(df_sub, "mean_events", "sem_events", "Firing Events")
  p2 <- plot_timecourse(df_sub, "mean_active", "sem_active", "% Active Frames")
  p3 <- plot_timecourse(df_sub, "mean_amp", "sem_amp", "Spike Amplitude")

  pdf(file.path(ca_results_dir, paste0("time_binned_metrics_", tag, ".pdf")), width = 6, height = 6)
  print(p1)
  print(p2)
  print(p3)
  dev.off()
}



# -------------------------------------------------
# plot low / high corr filtered time-binned metrics
# -------------------------------------------------
# filtered event rates 
p_bin_events_high <- plot_timecourse(
  plot_df_split %>% filter(is_high_corr == TRUE),
  "mean_events",
  "sem_events",
  "Firing Events"
)

p_bin_events_low <- plot_timecourse(
  plot_df_split %>% filter(is_high_corr == FALSE),
  "mean_events",
  "sem_events",
  "Firing Events"
)

# filtered activity rates 
p_bin_active_high <- plot_timecourse(
  plot_df_split %>% filter(is_high_corr == TRUE),
  "mean_active",
  "sem_active",
  "% Active Frames"
)

p_bin_active_low <- plot_timecourse(
  plot_df_split %>% filter(is_high_corr == FALSE),
  "mean_active",
  "sem_active",
  "% Active Frames"
)

# filtered spike amplitude 
p_bin_amp_high <- plot_timecourse(
  plot_df_split %>% filter(is_high_corr == TRUE),
  "mean_amp",
  "sem_amp",
  "Spike Amplitude"
)

p_bin_amp_low <- plot_timecourse(
  plot_df_split %>% filter(is_high_corr == FALSE),
  "mean_amp",
  "sem_amp",
  "Spike Amplitude"
)


# Extract time-binned plots
p_bin_events_high   <- p_bin_events_high
p_bin_events_low    <- p_bin_events_low

p_bin_active_high   <- p_bin_active_high
p_bin_active_low    <- p_bin_active_low

p_bin_amp_high      <- p_bin_amp_high
p_bin_amp_low       <- p_bin_amp_low

```


## IIIh. Combined Summary Figure & export of all values 
```{r}
# ---------------------------------------------------------------------------
# Create combined six‑panel figure (high and low corr separately)
# ---------------------------------------------------------------------------

# High correlation summary
neuro_combinedplot_high <-
  ((p_fire_norm_high | p_active_norm_high | p_amp_orig_high) /
   (p_bin_amp_high | p_bin_events_high | p_comm_high$p)) +
  plot_layout(heights = c(1, 1)) &
  theme(plot.margin = margin(5, 10, 5, 10))

ggsave(file.path(ca_results_dir, "neuro_combinedplot_high.pdf"),
       neuro_combinedplot_high, width = 12, height = 12, device = cairo_pdf)

# Low correlation summary
neuro_combinedplot_low <-
  ((p_fire_norm_low | p_active_norm_low | p_amp_orig_low) /
   (p_bin_amp_low | p_bin_events_low | p_comm_low$p)) +
  plot_layout(heights = c(1, 1)) &
  theme(plot.margin = margin(5, 10, 5, 10))

ggsave(file.path(ca_results_dir, "neuro_combinedplot_low.pdf"),
       neuro_combinedplot_low, width = 12, height = 12, device = cairo_pdf)


# ---------------------------------------------------------------------------
# Replace ANOVA + t-tests with Kruskal-Wallis + Wilcoxon
# ---------------------------------------------------------------------------
comm_df <- comm_df %>%
  mutate(ObjectNumber = as.character(ObjectNumber))

events_df <- events_df %>%
  mutate(ObjectNumber = as.character(ObjectNumber))

comm_df <- comm_df %>%
  left_join(events_df %>% select(Sample, ObjectNumber, is_high_corr),
            by = c("Sample", "ObjectNumber"))


calc_nonparametric_stats <- function(df, metric_col, group_col = "Genotype") {
  df <- df %>%
    dplyr::filter(!is.na(.data[[metric_col]]), !is.na(.data[[group_col]])) %>%
    dplyr::mutate(!!group_col := factor(.data[[group_col]]))

  if (length(unique(df[[group_col]])) < 2) {
    return(tibble(
      Metric = metric_col,
      group1 = "Kruskal-Wallis",
      group2 = "",
      p.value = NA_real_,
      sig_label = "NA"
    ))
  }

  kruskal <- kruskal.test(reformulate(group_col, metric_col), data = df)
  kw_pval <- kruskal$p.value

  pairwise <- pairwise.wilcox.test(df[[metric_col]], df[[group_col]], p.adjust.method = "BH")
  pairwise_df <- as.data.frame(as.table(pairwise$p.value)) %>%
    rename(group1 = Var1, group2 = Var2, p.value = Freq) %>%
    filter(!is.na(p.value)) %>%
    mutate(Metric = metric_col) %>%
    select(Metric, group1, group2, p.value)

  all_pvals <- bind_rows(
    tibble(Metric = metric_col, group1 = "Kruskal-Wallis", group2 = "", p.value = kw_pval),
    pairwise_df
  ) %>%
    mutate(sig_label = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01  ~ "**",
      p.value < 0.05  ~ "*",
      TRUE            ~ "ns"
    ))

  return(all_pvals)
}
# ---------------------------------------------------------------------------
# Combine and save all nonparametric p-values
# ---------------------------------------------------------------------------
all_pvals_high <- bind_rows(
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == TRUE), "events_per_min_norm"),
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == TRUE), "percent_active"),
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == TRUE), "spike_amplitude"),
  calc_nonparametric_stats(comm_df   %>% filter(is_high_corr == TRUE), "CommunitySize")
) %>% mutate(Source = "high_corr_ROIs")

all_pvals_low <- bind_rows(
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == FALSE), "events_per_min_norm"),
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == FALSE), "percent_active"),
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == FALSE), "spike_amplitude"),
  calc_nonparametric_stats(comm_df   %>% filter(is_high_corr == FALSE), "CommunitySize")
) %>% mutate(Source = "low_corr_ROIs")

all_pvals_combined <- bind_rows(all_pvals_high, all_pvals_low)

write.csv(all_pvals_combined,
          file = file.path(ca_results_dataframes, "calcium_metrics_pvals_high_vs_low.csv"),
          row.names = FALSE)
```
