---
title: "cellposeSAM_CNER_Calcium-liveCell"
output: html_document
date: "2025-05-20"
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# I. Load packackes
```{r}
library(devtools)

# Core data manipulation & visualization
library(tidyverse)        # ggplot2, dplyr, purrr, readr, tibble, tidyr, stringr
library(zoo)              # Rolling operations
library(reshape2)         # Data reshaping
library(pheatmap)         # Heatmaps
library(ggdendro)         # Dendrograms
library(viridis)          # Color palettes
library(RColorBrewer)
library(NatParksPalettes)
library(tidyplots)
library(ggpubr)           # Publication-ready plots
library(ggsignif)         # Significance annotations
library(Cairo)            # For cairo_pdf output
library(patchwork)        # Plot layouts
library(cowplot)          # Plot composition
library(gridExtra)        # Layout utilities
library(grid)             # Viewports and custom layout
library(rlang)            # Tidy evaluation tools

# Network analysis
library(igraph)
library(ggraph)
#devtools::install_github("simo-91/CalNetExploreR")
library(CalNetExploreR)

# Signal processing
#library(signal)

# Plot composition
library(gridExtra)
library(grid)
library(cowplot)
library(RColorBrewer)

# PCA visualization
library(factoextra)

```

# II. Preprocessing & Analysis
=> cellposeSAM based evaluation %>% into batch dF/F processing script 
=> used cellposeSAM for image segementation and measurement of ROIs
=> cellposeSAM output:
- each row is one soma followed by mean intensities saved across columns 
- each new timepoint is new column 

## IIa. User Input & Configuration
```{r}
# ==========================================================
# CellposeSAM-based Calcium Imaging Batch Analysis Pipeline
# ==========================================================
# This script integrates output from CellposeSAM, processes dF/F traces, 
# binarizes events, computes correlation-based networks (CNER), and 
# extracts per-cell statistics.


# Paths, user-defined parameters, sample/genotype mapping, colors
base_path <- "/Users/felix/Desktop/cpdemo/nd2_labels/"

# genotype_map <- list(
#   "iDA_Ctrl_d33_Flou4_100ms"        = "Control",
#   "iDA_Ctrl_d33_Flou4_90mM-KCL"     = "ControlKCL",
#   "iDA_ASAH1_d33_Flou4_100ms"       = "ASAH1",
#   "iDA_ASAH1_d33_Flou4_90mM-KCL"    = "ASAH1KCL"
# )

genotype_map <- list(
  "iN_d36_Ctrl_Fed_Fura4-LysoRed"        = "Control",
  "iN_d36_ASAH1_Fed_Fura4-LysoRed"       = "ASAH1"

)
# Direct mapping (no interpolation)
# Colors auto-generated based on mapping
geno_levels <- unique(unname(genotype_map))
base_colors <- c("grey20", "grey60", "skyblue", "navy")
stopifnot(length(base_colors) >= length(geno_levels))  # sanity check

geno_cols <- setNames(base_colors[seq_along(geno_levels)], geno_levels)


# Analysis parameters
baseline_window <- 25     # Rolling window size for baseline (frames)  
frame_rate      <- 10
smoothing_k     <- 1      # Optional smoothing over raw intensity traces (frames), set to NULL to disable smoothing
threshold_factor <- 0.2   # Threshold for binarization (fraction of SD)
correlation_threshold <- 0.2  #Correlation threshold for network (used in make_network())
image_dim <- c(1570, 1140)  # width, height in pixels of image. Need to backmap the roi activity network onto the image coordinats 

```

## IIb. ∆F/F Processing
```{r}
# -----------------------------
# Helper Function - ΔF/F
# -----------------------------
# Approach: Rolling Percentile + Edge-Safe
calculate_dff <- function(trace, window = baseline_window, smoothing_k = NULL, percentile = 0.2) {
  if (!is.null(smoothing_k)) {
    trace <- zoo::rollmean(trace, k = smoothing_k, fill = NA)
  }

  # Handle short or NA-filled traces
  if (length(trace) < window || all(is.na(trace))) {
    return(rep(NA, length(trace)))
  }

  baseline <- zoo::rollapply(trace, width = window,
    FUN = function(x) quantile(x, probs = percentile, na.rm = TRUE),
    align = "center", fill = NA, partial = TRUE)  # <- partial allows shorter edges

  baseline <- pmax(baseline, 1e-3)  # Avoid division by zero
  dff <- (trace - baseline) / baseline
  pmax(dff, 0)  # Clip negatives
}

# -----------------------------
# File Loader + Merger
# -----------------------------

calculate_dff <- function(trace, window = baseline_window, smoothing_k = NULL, percentile = 0.2) {
  if (!is.null(smoothing_k)) {
    trace <- zoo::rollmean(trace, k = smoothing_k, fill = NA)
  }
  if (length(trace) < window || all(is.na(trace))) {
    return(rep(NA, length(trace)))
  }
  baseline <- zoo::rollapply(trace, width = window,
    FUN = function(x) quantile(x, probs = percentile, na.rm = TRUE),
    align = "center", fill = NA, partial = TRUE)
  baseline <- pmax(baseline, 1e-3)
  dff <- (trace - baseline) / baseline
  pmax(dff, 0)
}

# Load and parse each centroid + intensity file pair
process_pair <- function(centroid_file) {
  intensity_file <- sub("_centroids_valid.csv$", "_intensity.csv", centroid_file)
  if (!file.exists(intensity_file)) return(NULL)

  sample_name <- basename(centroid_file)

  centroids <- read_csv(centroid_file, show_col_types = FALSE) %>%
    mutate(ObjectNumber = as.integer(label), file = sample_name) %>%
    select(file, ObjectNumber, centroid_x, centroid_y)

  intensity <- read_csv(intensity_file, show_col_types = FALSE) %>%
    select(-file)  # Drop if present

  traces_dff <- intensity %>%
    mutate(across(-time, ~calculate_dff(.x, window = baseline_window, smoothing_k = smoothing_k)))

  traces_long <- traces_dff %>%
    mutate(ImageNumber = row_number()) %>%
    pivot_longer(cols = -c(time, ImageNumber),
                 names_to = "ObjectNumber",
                 values_to = "Intensity_IntegratedIntensity_Fura",
                 values_drop_na = TRUE) %>%
    mutate(ObjectNumber = as.integer(ObjectNumber),
           file = sample_name)

  list(traces = traces_long, centroids = centroids)
}

# Load all files
file_list <- list.files(base_path, pattern = "_centroids_valid.csv$", full.names = TRUE)
parsed_data <- map(file_list, process_pair) %>% compact()

combined_df  <- map_dfr(parsed_data, "traces")
centroid_df  <- map_dfr(parsed_data, "centroids")

# -----------------------------
# Global dF/F Processing
# -----------------------------
df_processed <- combined_df %>%
  left_join(centroid_df, by = c("file", "ObjectNumber")) %>%
  group_by(file, ObjectNumber) %>%
  arrange(ImageNumber, .by_group = TRUE) %>%
  mutate(
    dF_F = calculate_dff(Intensity_IntegratedIntensity_Fura,
                         window = baseline_window,
                         smoothing_k = smoothing_k,
                         percentile = 0.2),
    dF_F_scaled = dF_F / max(dF_F, na.rm = TRUE),
    dF_F_binary = ifelse(dF_F > threshold_factor * sd(dF_F, na.rm = TRUE), 1, 0),
    spike_amplitude = ifelse(dF_F_binary == 1, dF_F_scaled, NA)
  ) %>%
  ungroup()

# -----------------------------
# Initialize Result Lists
# -----------------------------
cor_list <- list()
comm_list <- list()
events_list <- list()

summary_list <- list()

```



## IIc. Trace Plotting (per sample), Correlations and Firing Statistics 
```{r}
# --- create new sub-output dir for traces
trace_dir <- file.path(base_path, "label_traces")
dir.create(trace_dir, recursive = TRUE, showWarnings = FALSE)

# -----------------------------
# Per-Sample Processing
# -----------------------------
all_sample_ids <- unique(combined_df$file)

for (sample_id in all_sample_ids) {
  clean_sample_id <- gsub("_centroids_valid.csv$", "", basename(sample_id))
  output_dir <- file.path(base_path, clean_sample_id)
  dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)

  df_sample <- dplyr::filter(combined_df, file == sample_id)
  df_sample <- df_sample %>%
  select(-matches("^centroid_")) %>%
  left_join(centroid_df, by = c("file", "ObjectNumber"))
  
    df_processed <- df_sample %>%
      group_by(ObjectNumber) %>%
      arrange(ImageNumber) %>%
      mutate(
        dF_F = calculate_dff(Intensity_IntegratedIntensity_Fura,
                             window = baseline_window,
                             smoothing_k = smoothing_k,
                             percentile = 0.2),
        dF_F_scaled = dF_F / max(dF_F, na.rm = TRUE),
        dF_F_binary = ifelse(dF_F > threshold_factor * sd(dF_F, na.rm = TRUE), 1, 0),
        spike_amplitude = ifelse(dF_F_binary == 1, dF_F_scaled, NA)
      ) %>%
      ungroup()

  ## -----------------------------
  ## Trace plotting -- Iterate over each cell and save trace plots
  ## -----------------------------
  roi_ids <- unique(df_processed$ObjectNumber)
  for (i in seq_along(roi_ids)) {
    roi <- roi_ids[i]
    df_trace <- dplyr::filter(df_processed, ObjectNumber == roi)
  
    p_raw    <- ggplot(df_trace, aes(x = ImageNumber, y = Intensity_IntegratedIntensity_Fura)) + 
                geom_line(color = "black") + theme_minimal()
    p_scaled <- ggplot(df_trace, aes(x = ImageNumber, y = dF_F_scaled)) + 
                geom_line(color = "dodgerblue") + theme_minimal()
    p_binary <- ggplot(df_trace, aes(x = ImageNumber, y = dF_F_binary)) + 
                geom_step(color = "firebrick2") + theme_minimal()
  
    filename <- paste0(clean_sample_id, "_ROI_", i, ".pdf")
    ggsave(file.path(trace_dir, filename),
           p_raw / p_scaled / p_binary,
           width = 6, height = 6, device = cairo_pdf)
  }
    
  ## -----------------------------
  ## Calculation of Correlation matrix
  ## -----------------------------
  binary_matrix <- df_processed %>%
    select(ObjectNumber, ImageNumber, dF_F_binary) %>%
    pivot_wider(names_from = ImageNumber, values_from = dF_F_binary) %>%
    arrange(ObjectNumber)

  mat <- as.matrix(binary_matrix %>% select(-ObjectNumber))
  rownames(mat) <- binary_matrix$ObjectNumber
  corr <- cor(t(mat), use = "pairwise.complete.obs")
  corr[!is.finite(corr)] <- 0

  pdf(file.path(output_dir, paste0(clean_sample_id, "_CorrelationMatrix.pdf")), width = 6, height = 6)
  pheatmap(corr, clustering_method = "complete", main = "Correlation Matrix", border_color = NA, fontsize = 6)
  dev.off()

  ## -----------------------------
  ## Firing statistics 
  ## -----------------------------
firing_stats <- df_processed %>%
  arrange(ObjectNumber, ImageNumber) %>%
  group_by(ObjectNumber) %>%
  mutate(rising_edge = dF_F_binary == 1 & lag(dF_F_binary, default = 0) == 0) %>%
  summarise(
    firing_events   = sum(rising_edge, na.rm = TRUE),
    total_frames    = length(dF_F_binary),
    active_frames   = sum(dF_F_binary, na.rm = TRUE),
    percent_active  = 100 * active_frames / total_frames,
    spike_amplitude = mean(dF_F_scaled[dF_F_binary == 1], na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    Sample = sample_id,
    Group = ifelse(grepl("Ctrl", sample_id, ignore.case = TRUE), "Control", "ASAH1")
  )

  events_list[[clean_sample_id]] <- firing_stats

  
  
  ## -------- A.  constants for this movie --------------------------------
  n_cells        <- n_distinct(df_processed$ObjectNumber)
  n_frames_total <- max(df_processed$ImageNumber)          # assumes no gaps
  duration_min   <- n_frames_total / (frame_rate*60)       # movie length (min)
  
  ## -------- B.  add *per‑cell* firing‑rate‑per‑min ----------------------
  firing_stats <- firing_stats %>%
    mutate(firing_rate_per_min =
             firing_events / duration_min)   #  normalised for time
  
  ## -------- C.  *sample‑level* aggregates, normalised for # labels ------
  sample_summary <- firing_stats %>%
    summarise(
      Sample                       = clean_sample_id,
      Total_cells                  = n_cells,
      ## --- firing events -----------------------------------------------
      Total_firing_events          = sum(firing_events, na.rm = TRUE),
      Firing_events_per_min        = Total_firing_events / duration_min,
      ##   …and now      ÷ number of ROIs
      Firing_events_per_min_norm   = Firing_events_per_min / n_cells,
      ## --- activity -----------------------------------------------------
      Cells_active                 = sum(firing_events > 0, na.rm = TRUE),
      Percent_cells_active         = 100 * Cells_active / n_cells,
      ##   per‑ROI normalisation of “% active frames” metric
      Mean_percent_active_frames   = mean(percent_active, na.rm = TRUE),
      Mean_percent_active_frames_norm = Mean_percent_active_frames      # already
    )
  
  ## -------- D.  stash for later -----------------------------------------
  summary_list[[clean_sample_id]] <- sample_summary
    
  
  

  # --- Save per-sample outputs ---
  write.csv(firing_stats, file.path(output_dir, paste0(clean_sample_id, "_firing_stats.csv")), row.names = FALSE)
  write.csv(df_processed,  file.path(output_dir, paste0(clean_sample_id, "_df_processed.csv")), row.names = FALSE)
}

```

## 

``` {r}
# -------------------------------------------------
# CNER Network Construction (per sample)
# -------------------------------------------------
# --- Parameters and directories
output_dir <- file.path(base_path, clean_sample_id)
dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)

image_dim <- c(1570, 1140)
correlation_threshold <- 0.3

# --- Ensure matching types and overlap
centroid_df_clean <- centroid_df %>%
  mutate(file = as.character(file), ObjectNumber = as.integer(ObjectNumber))

# Prepare data with centroids
df_cner_input <- combined_df %>%
  mutate(file = as.character(file), ObjectNumber = as.integer(ObjectNumber)) %>%
  filter(file %in% centroid_df_clean$file) %>%
  select(-matches("^centroid_")) %>%
  left_join(centroid_df_clean, by = c("file", "ObjectNumber")) %>%
  group_by(file, ObjectNumber) %>%
  arrange(ImageNumber, .by_group = TRUE) %>%
  mutate(
    dF_F = calculate_dff(Intensity_IntegratedIntensity_Fura,
                         window = baseline_window,
                         smoothing_k = smoothing_k,
                         percentile = 0.2),
    dF_F_scaled = dF_F / max(dF_F, na.rm = TRUE),
    dF_F_binary = ifelse(dF_F > threshold_factor * sd(dF_F, na.rm = TRUE), 1, 0),
    spike_amplitude = ifelse(dF_F_binary == 1, dF_F_scaled, NA)
  ) %>%
  ungroup()

# --- Filter one sample
sample_id <- unique(df_cner_input$file)[1]
df_cner_sample <- df_cner_input %>% filter(file == sample_id)

# --- Sanity check for coordinates
stopifnot(all(c("centroid_x", "centroid_y") %in% colnames(df_cner_sample)))

# --- Binary matrix (rows = ROIs, cols = time)
binary_matrix <- df_cner_sample %>%
  select(ObjectNumber, ImageNumber, dF_F_binary) %>%
  pivot_wider(names_from = ImageNumber, values_from = dF_F_binary) %>%
  arrange(ObjectNumber)

mat <- as.matrix(binary_matrix %>% select(-ObjectNumber))
rownames(mat) <- binary_matrix$ObjectNumber

# --- Correlation matrix
cor_mat <- cor(t(mat), use = "pairwise.complete.obs")
cor_mat[!is.finite(cor_mat)] <- 0

# --- Extract edges above threshold
edge_list <- which(cor_mat > correlation_threshold & upper.tri(cor_mat), arr.ind = TRUE)
edges <- data.frame(
  from = rownames(cor_mat)[edge_list[, 1]],
  to   = rownames(cor_mat)[edge_list[, 2]],
  corr = cor_mat[edge_list]
)

# --- Extract coordinates
coords <- df_cner_sample %>%
  filter(!is.na(centroid_x), !is.na(centroid_y)) %>%
  group_by(ObjectNumber) %>%
  slice(1) %>%
  ungroup() %>%
  distinct(ObjectNumber, centroid_x, centroid_y) %>%
  mutate(Label = as.character(ObjectNumber))






# Color scale
corr_colors <- colorRampPalette(RColorBrewer::brewer.pal(11, "RdYlBu"))(100)
edges$corr_clipped <- pmin(pmax(edges$corr, 0.3), 1)

# Map correlation to color index manually
corr_vals <- seq(0.3, 1, length.out = 100)
color_idx <- findInterval(edges$corr_clipped, corr_vals, all.inside = TRUE)
edge_cols <- corr_colors[color_idx]

pdf(file.path(output_dir, "network_from_centroids.pdf"), width = 6.5, height = 6)
layout(matrix(c(1, 2), nrow = 1), widths = c(5, 1.5))
par(mar = c(4, 4, 2, 1))

# Main plot
plot(coords$centroid_x, coords$centroid_y, pch = 21, bg = "black", col = "white",
     xlab = "X", ylab = "Y", main = sample_id, asp = 1)
for (i in seq_len(nrow(edges))) {
  p1 <- coords %>% filter(Label == edges$from[i])
  p2 <- coords %>% filter(Label == edges$to[i])
  if (nrow(p1) == 1 && nrow(p2) == 1) {
    segments(p1$centroid_x, p1$centroid_y, p2$centroid_x, p2$centroid_y,
             col = edge_cols[i], lwd = 1)
  }
}

# Create a color bar image
legend_img <- as.raster(matrix(corr_colors, ncol = 1))


# Plot the legend
par(mar = c(4, 2, 2, 4))
plot(NA, xlim = c(0, 1), ylim = c(0.3, 1), type = "n", axes = FALSE, xlab = "", ylab = "")
rasterImage(legend_img, 0, 0.3, 1, 1)
axis(4, at = seq(0.3, 1, 0.2), las = 1)
mtext("Correlation", side = 4, line = 2.5)

dev.off()



```

## IId. Save combined Outputs & Sample-Level Normalized Outputs
```{r}
# -----------------------------
# Save Combined Outputs
# -----------------------------
write.csv(bind_rows(cor_list),    file.path(base_path, "combined_cell_pairwise_correlations.csv"), row.names = FALSE)
write.csv(bind_rows(comm_list),   file.path(base_path, "combined_cell_community_membership.csv"),  row.names = FALSE)
write.csv(bind_rows(events_list), file.path(base_path, "combined_events_per_min_results.csv"),     row.names = FALSE)


# ----------------------------------------------------------------------
# Save sample‑level, normalized metrics
# ----------------------------------------------------------------------
normalised_metrics <- bind_rows(summary_list)

write.csv(normalised_metrics,
          file.path(base_path,
                    "combined_sample_level_normalised_metrics.csv"),
          row.names = FALSE)


```




# III.  Plotting & Statistical Evaluation
## IIIa. ROI Couting and Event Rates Calculation 
```{r}
## ---------------------------------------------------------------------------
##  Plot cellposeSAM/CNER results  –incl. events per min^-1 (±SEM)
## ---------------------------------------------------------------------------

## helper: bind & be sure a “Sample” column exists ---------------------------
bind_with_sample <- function(lst) {
  ## imap_dfr() gives you each element (.x) *and* its list name (.y)
  imap_dfr(lst, ~ {
    if (!"Sample" %in% names(.x))
      .x <- mutate(.x, Sample = .y)  # add from list‑element name
    .x
  })
}

# ----------------------------------------------------------------------
# collapse lists to data‑frames (guaranteed to have ‘Sample’)
# ----------------------------------------------------------------------
events_df <- bind_with_sample(events_list)
comm_df   <- bind_with_sample(comm_list)

# sanity‑check ―now TRUE
stopifnot("Sample" %in% names(events_df),
          "Sample" %in% names(comm_df))

# ----------------------------------------------------------------------
# add genotype label
# ----------------------------------------------------------------------
add_genotype <- function(df, mapping) {
  df %>%
    mutate(Genotype = map_chr(Sample, function(s) {
      matched <- names(mapping)[str_detect(s, fixed(names(mapping)))]
      if (length(matched) > 0) mapping[[matched[1]]] else NA_character_
    })) %>%
    drop_na(Genotype)
}
events_df <- add_genotype(events_df, genotype_map)
comm_df   <- add_genotype(comm_df, genotype_map)
normalised_metrics <- add_genotype(normalised_metrics, genotype_map)
#dat <- add_genotype(dat, genotype_map)

# ----------------------------------------------------------------------
# ROI count & calculation of (un)normalised firing–rate
# ----------------------------------------------------------------------

events_df <- events_df %>% 
  group_by(Sample) %>% 
  mutate(
    n_labels          = n(),                             # ROI count
    events_per_min    = firing_events /
                        (total_frames / (frame_rate * 60)),
    events_per_min_norm = events_per_min / n_labels
  ) %>% 
  ungroup()

unique_genos <- unique(c(events_df$Genotype, comm_df$Genotype))
geno_cols <- setNames(colorRampPalette(base_colors)(length(unique_genos)), unique_genos)



```



## IIIb. Scatterplots / Violinplots with SEM (per genotype)
```{r}
# ----------------------------------------------------------------------
# Helper function: plotting violinplot with scatter+SEM error‑bars
# ----------------------------------------------------------------------
plot_metric <- function(dat, metric, ylab, outfile, base_dir) {
  require(ggpubr)

  # --- check that metric exists
  if (!metric %in% names(dat)) {
    stop(paste("Metric", metric, "not found in data"))
  }

  metric_sym <- rlang::sym(metric)

  # --- Ensure factor order
  dat <- dat %>%
    mutate(Genotype = factor(Genotype,
                             levels = unique(unname(genotype_map))))

  # --- Group stats
  mean_dat <- dat %>%
    group_by(Genotype) %>%
    summarise(
      mean_val = mean(!!metric_sym, na.rm = TRUE),
      sem      = sd(!!metric_sym, na.rm = TRUE) / sqrt(n()),
      .groups  = "drop"
    )

  # --- Calculate ANOVA
  aov_model <- aov(as.formula(paste0(metric, " ~ Genotype")), data = dat)
  aov_pval  <- summary(aov_model)[[1]][["Pr(>F)"]][1]

  # --- Pairwise comparisons
  filtered_dat <- dplyr::filter(dat, !is.na(!!metric_sym))
  pw <- pairwise.t.test(filtered_dat[[metric]], filtered_dat$Genotype, p.adjust.method = "holm")

  if (!is.null(pw$p.value)) {
    pairwise_df <- as.data.frame(as.table(pw$p.value), stringsAsFactors = FALSE) %>%
      dplyr::filter(!is.na(Freq) & Freq < 0.05) %>%
      dplyr::rename(group1 = Var1, group2 = Var2, p.value = Freq)

    if (nrow(pairwise_df) > 0) {
      pairwise_df <- pairwise_df %>%
        mutate(
          y.position = seq(from = max(dat[[metric]], na.rm = TRUE) * 1.05,
                           by   = max(dat[[metric]], na.rm = TRUE) * 0.05,
                           length.out = n()),
          p.adj = signif(p.value, 2),
          group1 = factor(group1, levels = levels(dat$Genotype)),
          group2 = factor(group2, levels = levels(dat$Genotype))
        )
    }
  } else {
    pairwise_df <- tibble(group1 = character(), group2 = character(), p.value = numeric())
  }

  # --- Plot violin plot 
p <- ggplot(dat, aes(x = Genotype, y = !!metric_sym, fill = Genotype)) +
  geom_jitter(color = "grey80", width = 0.1, size = 0.5, alpha = 0.5) +
  geom_violin(scale = "width", trim = FALSE, color = NA, width = 0.85, adjust = 1.5) +
  geom_boxplot(width = 0.2, fill = "white", outlier.shape = NA, color = "black", linewidth = 0.3) +
  scale_fill_manual(values = geno_cols) +
  labs(y = ylab, x = NULL) +
  theme_bw(base_size = 6) +
  theme(legend.position = "none",
        aspect.ratio = 2,
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

  
  # --- Add pairwise significance bars
  if (nrow(pairwise_df) > 0) {
    p <- p + stat_pvalue_manual(pairwise_df,
                                label = "p.adj",
                                y.position = "y.position",
                                xmin = "group1",
                                xmax = "group2",
                                tip.length = 0.01,
                                size = 2,
                                inherit.aes = FALSE)
  }

  # --- Save plot
  ggsave(file.path(base_path, paste0(outfile, ".pdf")),
         p, width = 2, height = 5, device = cairo_pdf)

  # --- Return plot + pvals
  list(
    plot = p,
    pval = bind_rows(
      tibble(Metric = metric, group1 = "ANOVA", group2 = "", p.value = aov_pval),
      pairwise_df %>% mutate(Metric = metric) %>%
        select(Metric, group1, group2, p.value)
    )
  )
}

# ---------------------------------------------------------------------------
# Save general, per‑cell metrics
# ---------------------------------------------------------------------------
# Capture p-values for Export
p_fire_orig_result   <- plot_metric(events_df, "firing_events", "Firing Events", "fire_events", base_path)
p_active_orig_result <- plot_metric(events_df, "percent_active", "% Active Frames", "percent_active", base_path)
p_amp_orig_result    <- plot_metric(events_df, "spike_amplitude", "Spike Amplitude", "spike_amplitude", base_path)

# extract plots
p_fire_orig   <- p_fire_orig_result$plot
p_active_orig <- p_active_orig_result$plot
p_amp_orig    <- p_amp_orig_result$plot



```


## IIIc. Sample-Level Normalized Metrics
```{r}
# ---------------------------------------------------------------------------
# Sample‑level normalised metrics from `normalised_metrics`
# ---------------------------------------------------------------------------

p_fire_norm_result <- plot_metric(normalised_metrics,
                                  "Firing_events_per_min_norm",
                                  "Events · min⁻¹ / ROI",
                                  "fire_events_norm", base_path)

p_active_norm_result <- plot_metric(normalised_metrics,
                                    "Mean_percent_active_frames_norm",
                                    "% Active / ROI",
                                    "percent_active_norm", base_path)

p_fire_norm   <- p_fire_norm_result$plot
p_active_norm <- p_active_norm_result$plot


```



## IIId. Community Size Analysis & ANOVA tests 
```{r}
# ---------------------------------------------------------------------------
# Community‑size plot with ANOVA + pairwise stats
# ---------------------------------------------------------------------------
comm_df <- add_genotype(comm_df, genotype_map) %>%
  mutate(Genotype = factor(Genotype, levels = names(geno_cols)))

comm_df <- comm_df %>%
  mutate(CommunitySize = as.numeric(CommunitySize))

comm_dat <- comm_df %>%
  group_by(Genotype) %>%
  summarise(mean_size = mean(CommunitySize, na.rm = TRUE),
            sem       = sd(CommunitySize, na.rm = TRUE) / sqrt(n()),
            .groups   = "drop")

# --- Global ANOVA
comm_aov <- aov(CommunitySize ~ Genotype, data = comm_df)
comm_aov_pval <- summary(comm_aov)[[1]][["Pr(>F)"]][1]

# --- Pairwise comparisons
comm_pw <- pairwise.t.test(comm_df$CommunitySize, comm_df$Genotype, p.adjust.method = "holm")

if (is.null(comm_pw$p.value)) {
  comm_pw_df <- tibble(group1 = character(0),
                       group2 = character(0),
                       p.value = numeric(0))
} else {
  comm_pw_df <- as.data.frame(as.table(comm_pw$p.value), stringsAsFactors = FALSE) %>%
    dplyr::filter(!is.na(Freq) & Freq < 0.05) %>%
    dplyr::rename(group1 = Var1, group2 = Var2, p.value = Freq)
}

# --- Add y positions and format annotations
if (nrow(comm_pw_df) > 0) {
  comm_pw_df <- comm_pw_df %>%
    mutate(
      y_position = seq(from = max(comm_df$CommunitySize, na.rm = TRUE) * 1.05,
                       by = max(comm_df$CommunitySize, na.rm = TRUE) * 0.05,
                       length.out = n()),
      annotations = signif(p.value, 2)
    )
}

# --- Plot
p_comm <- ggplot(comm_df, aes(x = Genotype, y = CommunitySize, fill = Genotype)) +
  geom_jitter(color = "grey80", width = 0.1, size = 0.5, alpha = 0.5) +
  geom_violin(scale = "width", trim = FALSE, color = NA, width = 0.85, adjust = 1.5) +
  geom_boxplot(width = 0.2, fill = "white", outlier.shape = NA, color = "black", linewidth = 0.3) +
  scale_fill_manual(values = geno_cols) +
  labs(y = "Community Size", x = NULL) +
  theme_bw(base_size = 6) +
  theme(legend.position = "none",
        aspect.ratio    = 2,
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

# --- Add significance brackets manually
if (nrow(comm_pw_df) > 0) {
  comm_pw_df <- comm_pw_df %>%
    mutate(
      y.position = seq(from = max(comm_df$CommunitySize, na.rm = TRUE) * 1.05,
                       by = max(comm_df$CommunitySize, na.rm = TRUE) * 0.05,
                       length.out = n()),
      p.adj = signif(p.value, 2),
      group1 = factor(group1, levels = levels(comm_df$Genotype)),
      group2 = factor(group2, levels = levels(comm_df$Genotype))
    )

  p_comm <- p_comm +
    stat_pvalue_manual(comm_pw_df,
                       label = "p.adj",
                       y.position = "y.position",
                       xmin = "group1",
                       xmax = "group2",
                       tip.length = 0.01,
                       size = 2,
                       inherit.aes = FALSE)
}

# --- Save
ggsave(file.path(base_path, "plot_community_size.pdf"),
       p_comm, width = 2, height = 3, device = cairo_pdf)


```


## IIIe. Combined Summary Figure & export of all values 
```{r}
# ---------------------------------------------------------------------------
# Create combined six‑panel figure
# ---------------------------------------------------------------------------
neuro_combinedplot <-
  ((p_fire_orig | p_active_orig | p_amp_orig) /
   (p_fire_norm | p_active_norm | p_comm    )) +
  plot_layout(heights = c(1, 1)) &
  theme(plot.margin = margin(.01, 10, .01, 10))

ggsave(file.path(base_path, "neuro_combinedplot.pdf"),
       neuro_combinedplot, width = 6, height = 4, device = cairo_pdf)

# ---------------------------------------------------------------------------
# Combine and save all stat. values 
# ---------------------------------------------------------------------------

# Save All P-values in csv file in prev. defined outdir 
all_pvals <- bind_rows(
  p_fire_orig_result$pval,
  p_active_orig_result$pval,
  p_amp_orig_result$pval,
  p_fire_norm_result$pval,
  p_active_norm_result$pval,
  bind_rows(
  tibble(Metric = "CommunitySize", group1 = "ANOVA", group2 = "", p.value = comm_aov_pval),
  comm_pw_df %>% mutate(Metric = "CommunitySize") %>%
    select(Metric, group1, group2, p.value)
  )
)


all_pvals <- all_pvals %>%
  select(Metric, group1, group2, p.value)


write.csv(all_pvals,
          file = file.path(base_path, "calcium_metrics_pvals.csv"),
          row.names = FALSE)

```




