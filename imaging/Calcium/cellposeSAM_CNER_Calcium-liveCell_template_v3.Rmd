---
title: "cellposeSAM_CNER_Calcium-liveCell_v3"
output: html_document
date: "2025-07-17"
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# I. Load packackes
```{r}
library(devtools)

# Core data manipulation & visualization
library(tidyverse)        # ggplot2, dplyr, purrr, readr, tibble, tidyr, stringr
library(zoo)              # Rolling operations
library(reshape2)         # Data reshaping
library(pheatmap)         # Heatmaps
library(ggdendro)         # Dendrograms
library(viridis)          # Color palettes
library(RColorBrewer)
library(NatParksPalettes)
library(tidyplots)
library(ggpubr)           # Publication-ready plots
library(ggsignif)         # Significance annotations
library(Cairo)            # For cairo_pdf output
library(patchwork)        # Plot layouts
library(cowplot)          # Plot composition
library(gridExtra)        # Layout utilities
library(grid)             # Viewports and custom layout
library(rlang)            # Tidy evaluation tools

# Network analysis
library(igraph)
library(ggraph)
#devtools::install_github("simo-91/CalNetExploreR")
library(CalNetExploreR)

# Signal processing
#library(signal)

# Plot composition
library(gridExtra)
library(grid)
library(cowplot)
library(RColorBrewer)

# PCA visualization
library(factoextra)

library(dplyr)

```

# II. Preprocessing & Analysis
=> cellposeSAM based evaluation %>% into batch dF/F processing script 
=> used cellposeSAM for image segementation and measurement of ROIs
=> cellposeSAM output:
- each row is one soma followed by mean intensities saved across columns 
- each new timepoint is new column 

## IIa. User Input & Configuration
```{r}
# ==========================================================
# CellposeSAM-based Calcium Imaging Batch Analysis Pipeline
# ==========================================================
# This script integrates output from CellposeSAM, processes dF/F traces, 
# binarizes events, computes correlation-based networks (CNER), and 
# extracts per-cell statistics.

# -----------------------------------------
# BASE DIR & OUTPUT DIRs
# -----------------------------------------
# Path to root directory where input data and output folders are/will be stored
base_path <- "/Users/felix/HMS Dropbox/Felix Kraus/Felix/Harvard/03_LSD-PD/Microscopy/20250710_diff138_iN_d38_Ctrl-ASAH1_Flou4/cpsam_d38/test"

# Path to save all plots
ca_results_dir <- file.path(base_path, "calcium_analysis_results")
dir.create(ca_results_dir, showWarnings = FALSE, recursive = TRUE)

# define output dirs:
ca_results_dataframes <- file.path(base_path, "calcium_analysis_results_dataframes")
dir.create(ca_results_dataframes, recursive = TRUE, showWarnings = FALSE)




# -------------------
# USER-INPUT 1: SAMPLE INFO 
# -------------------

# USER INPUT HERE 
celltype <- "iN"
timepoint <- "d38"
genotypes <- c("Ctrl", "ASAH1")
treatments <- c("Fed_Fluo4", "100mMKCL_Fluo4","500µMCaffeine_Fluo4","25µMCQNX10µMDAP5_Fluo4")

# -------------------
# USER-INPUT 2: ANALYSIS PARAMETERS
# -------------------
baseline_window <- 50             # Rolling window size for baseline (frames).
frame_rate      <- 10
smoothing_k     <- 10            # Optional smoothing over raw intensity traces (frames), set to NULL to disable smoothing
correlation_threshold <- 0.4       # Correlation threshold for network (used in make_network())
threshold_factor <- 0.8 
roi_corr_threshold    <- 0.4       # for identifying synchronized ROIs (used in spike rate filtering)
image_dim <- c(785, 570)         # width, height in pixels of image. Need to backmap the roi activity network onto the image coordinats 
#image_dim <- c(1570, 1140)



# -------------------
# COLOR MAPPING (by SampleLabel)
# -------------------

# Construct sample_keys with SampleLabel logic
sample_keys <- expand.grid(
  Genotype  = genotypes,
  Treatment = treatments,
  stringsAsFactors = FALSE
) %>%
  mutate(
    SampleLabel = str_c(Genotype, Treatment, sep = "_"),
    PlotOrder = paste(Genotype, match(Treatment, treatments), sep = "_")
  ) %>%
  arrange(factor(Genotype, levels = genotypes), match(Treatment, treatments)) %>%
  mutate(SampleLabel = factor(SampleLabel, levels = SampleLabel))

# Color mapping for plots
geno_levels <- sample_keys$SampleLabel

geno_cols <- setNames(
  sapply(sample_keys$Genotype, function(geno) {
    if (geno == "Ctrl") {
      "grey70"
    } else if (geno == "ASAH1") {
      "lightblue"
    } else {
      "lightgrey"
    }
  }),
  sample_keys$SampleLabel
)

# Mapping from sample key (Celltype_Timepoint_Genotype_Treatment) to SampleLabel
genotype_map <- setNames(sample_keys$SampleLabel,
                         paste(celltype, timepoint, sample_keys$Genotype, sample_keys$Treatment, sep = "_"))


# # # Colors auto-generated based on mapping
# base_colors <- RColorBrewer::brewer.pal(length(geno_levels), "Set2")  
# stopifnot(length(base_colors) >= length(geno_levels))



```

## IIb. Helper Function – parse through paried intesnitsy / centroids and join in wide format
```{r}
# -----------------------------
# Helper Function – parse through paired intensity / centroids and join in wide format
# -----------------------------

# define output dirs:
ca_results_matched <- file.path(base_path, "01_matched_dfs")
dir.create(ca_results_matched, recursive = TRUE, showWarnings = FALSE)


# Find matching pairs by FileID
intensity_files <- list.files(base_path, pattern = "_intensity\\.csv$", full.names = TRUE)
centroid_files  <- list.files(base_path, pattern = "_centroids_valid\\.csv$", full.names = TRUE)

intensity_meta <- tibble(
  File = intensity_files,
  FileID = str_remove(basename(File), "_intensity\\.csv$")
)

centroids_meta <- tibble(
  File = centroid_files,
  FileID = str_remove(basename(File), "_centroids_valid\\.csv$")
)

# Process each matched pair
matched_ids <- intersect(intensity_meta$FileID, centroids_meta$FileID)

for (file_id in matched_ids) {
  message("Processing: ", file_id)

  # Load files
  int_file <- intensity_meta %>% filter(FileID == file_id) %>% pull(File)
  cen_file <- centroids_meta %>% filter(FileID == file_id) %>% pull(File)

  intensity_df <- read_csv(int_file, show_col_types = FALSE)
  centroids_df <- read_csv(cen_file, show_col_types = FALSE)

  if ("label" %in% names(centroids_df)) {
    centroids_df <- centroids_df %>% rename(ObjectNumber = label)
  }

  # Match ROI labels
  shared_labels <- intersect(
    colnames(intensity_df)[grepl("^[0-9]+$", colnames(intensity_df))],
    as.character(centroids_df$ObjectNumber)
  )
  if (length(shared_labels) == 0) next

  # Reshape intensity to wide format (rows = ROI, cols = time)
  intensity_long <- intensity_df %>%
    pivot_longer(cols = all_of(shared_labels), names_to = "ObjectNumber", values_to = "intensity") %>%
    mutate(ObjectNumber = as.integer(ObjectNumber)) %>%
    pivot_wider(names_from = time, values_from = intensity)

  # Merge and save
  merged_df <- centroids_df %>%
    filter(ObjectNumber %in% intensity_long$ObjectNumber) %>%
    left_join(intensity_long, by = "ObjectNumber") %>%
    mutate(FileID = file_id) %>%
    select(FileID, ObjectNumber, centroid_x, centroid_y, everything())

  out_file <- file.path(ca_results_matched, paste0(file_id, "_matched.csv"))
  write_csv(merged_df, out_file)
}


```

## IIc. Helper Function – parse through matched df folder and add metadata
```{r}
# -----------------------------
# Add metadata to matched CSVs
# -----------------------------
matched_dir <- file.path(base_path, "01_matched_dfs")
annotated_dir <- file.path(base_path, "02_annotated_dfs")
dir.create(annotated_dir, recursive = TRUE, showWarnings = FALSE)

matched_files <- list.files(matched_dir, full.names = TRUE, pattern = "_matched\\.csv$")

for (file in matched_files) {
  df <- readr::read_csv(file, show_col_types = FALSE)
  file_base <- str_remove(basename(file), "_matched\\.csv$")
  parts <- str_split(file_base, "_", simplify = TRUE)

  if (ncol(parts) < 6) {
    message("Skipping malformed filename: ", file)
    next
  }

  # Create metadata row including Sample
  meta_row <- tibble(
    FileID    = file_base,
    Celltype  = parts[, 1],
    Timepoint = parts[, 2],
    Genotype  = parts[, 3],
    Treatment = paste(parts[, 4], parts[, 5], sep = "_"),
    Replicate = parts[, 6]
  ) %>%
    mutate(Sample = paste(Celltype, Timepoint, Genotype, Treatment, sep = "_")) %>%
    relocate(Sample, .before = everything())  

  meta_clean <- meta_row %>%
    select(Sample, FileID, Celltype, Timepoint, Genotype, Treatment, Replicate) %>%
    select(-any_of(intersect(names(.), names(df))))

  # Bind metadata to each row
  annotated_df <- bind_cols(meta_clean[rep(1, nrow(df)), ], df)

  out_file <- file.path(annotated_dir, paste0(file_base, "_annotated.csv"))
  readr::write_csv(annotated_df, out_file)
}
```

## IId. Helper Function – parse through matched df folder and add metadata
```{r}
# -----------------------------
# Define paths
# -----------------------------
annotated_dir <- file.path(base_path, "02_annotated_dfs")
out_dir <- ca_results_dataframes

# -----------------------------
# Function: Build combined centroid and intensity data frames
# -----------------------------
build_centroid_intensity_dfs <- function(annotated_dir, out_dir) {
  annotated_files <- list.files(annotated_dir, pattern = "_annotated\\.csv$", full.names = TRUE)

  all_centroids <- list()
  all_intensities <- list()

  for (file in annotated_files) {
    df <- readr::read_csv(file, show_col_types = FALSE)

    # Identify timepoint columns (named "1", "2", ..., etc.)
    time_cols <- names(df)[grepl("^[0-9]+$", names(df))]
    if (length(time_cols) == 0) next

    # Metadata columns (Sample first)
    meta_cols <- c("Sample", "FileID", "Celltype", "Timepoint", "Genotype", "Treatment", "Replicate")

    # Extract centroid and intensity data
    centroid_df <- df %>%
      select(all_of(meta_cols), ObjectNumber, centroid_x, centroid_y)

    intensity_df <- df %>%
      select(all_of(meta_cols), ObjectNumber, all_of(time_cols))

    all_centroids[[file]] <- centroid_df
    all_intensities[[file]] <- intensity_df
  }

  centroid_df_all <- bind_rows(all_centroids)
  intensity_df_all <- bind_rows(all_intensities)

  write_csv(centroid_df_all, file.path(out_dir, "centroid_df.csv"))
  write_csv(intensity_df_all, file.path(out_dir, "intensity_df.csv"))

  return(list(
    centroid_df = centroid_df_all,
    intensity_df = intensity_df_all
  ))
}

# -----------------------------
# Run the function
# -----------------------------
combined_dfs <- build_centroid_intensity_dfs(annotated_dir, ca_results_dataframes)
centroid_df <- combined_dfs$centroid_df
intensity_df <- combined_dfs$intensity_df
```


## IIe. ∆F/F Processing
```{r}
# -----------------------------
# Helper Function – ΔF/F Calculation
# -----------------------------
calculate_dff <- function(trace, window = baseline_window, smoothing_k = NULL, percentile = 0.2) {
  if (!is.null(smoothing_k)) {
    trace <- zoo::rollmean(trace, k = smoothing_k, fill = NA)
  }

  if (length(trace) < window || all(is.na(trace))) {
    # Inject low-level noise if trace is unusable
    return(rnorm(length(trace), mean = 0, sd = 0.001))
  }

  baseline <- zoo::rollapply(trace, width = window,
                              FUN = function(x) quantile(x, probs = percentile, na.rm = TRUE),
                              align = "center", fill = NA, partial = TRUE)

  if (all(is.na(baseline))) {
    baseline <- rep(1e-3, length(trace))  # fallback: flat baseline
  } else {
    min_valid <- min(baseline, na.rm = TRUE)
    baseline[is.na(baseline)] <- max(min_valid, 1e-3)
  }

  baseline <- pmax(baseline, 1e-3)
  dff <- (trace - baseline) / baseline
  pmax(dff, 0)
}
# -----------------------------
# Helper Function – Noise Filter + dF/F from in-memory data
# -----------------------------
process_pair <- function(file_id, intensity_df, centroid_df) {
  intensity_sub <- intensity_df %>% filter(FileID == file_id)
  centroid_sub <- centroid_df %>% filter(FileID == file_id)

  if (nrow(intensity_sub) == 0 || nrow(centroid_sub) == 0) return(NULL)

  time_cols <- names(intensity_sub)[grepl("^[0-9]+$", names(intensity_sub))]

  baseline_avg <- intensity_sub %>%
    select(all_of(time_cols)) %>%
    as.matrix() %>%
    .[1, ] %>%
    median(na.rm = TRUE)

  filter_thresh <- 0.3 * baseline_avg

  intensity_filtered <- intensity_sub %>%
    mutate(across(all_of(time_cols), ~pmax(.x, filter_thresh)))

  traces_dff <- intensity_filtered %>%
    mutate(across(all_of(time_cols), ~calculate_dff(.x, window = baseline_window*0.25, smoothing_k = NULL, percentile = 0.2)))

  # Identify and inject noise into flat ΔF/F traces
  traces_dff[time_cols] <- traces_dff[time_cols] %>%
    as.matrix() %>%
    apply(1, function(row) {
      if (sd(row, na.rm = TRUE) < 1e-6 || all(row == 0, na.rm = TRUE)) {
        rnorm(length(row), mean = 0, sd = 0.001)
      } else {
        row
      }
    }) %>%
    t() %>%
    as_tibble(.name_repair = "minimal")
  
  # Drop duplicated metadata columns from centroid_sub before join
  centroid_meta <- centroid_sub %>%
    select(FileID, ObjectNumber, centroid_x, centroid_y)

  # Now join and select
  traces_dff <- traces_dff %>%
    left_join(centroid_meta, by = c("FileID", "ObjectNumber")) %>%
    select(Sample, FileID, Celltype, Timepoint, Genotype, Treatment, Replicate,
           ObjectNumber, centroid_x, centroid_y, all_of(time_cols))

  return(traces_dff)
}

# -----------------------------
# Run dF/F processing across all samples
# -----------------------------
dff_results <- map(unique(intensity_df$FileID), ~process_pair(.x, intensity_df, centroid_df)) %>%
  compact() %>%
  bind_rows()

write_csv(dff_results, file = file.path(ca_results_dataframes, "dff_results.csv"))


# -----------------------------
# Initialize Result Lists
# -----------------------------
cor_list <- list()
comm_list <- list()
events_list <- list()
summary_list <- list()
```



## IIf. Trace Plotting (per sample), Correlations, Network plots and Firing Statistics 
```{r}
# Setup
trace_dir <- file.path(base_path, "label_traces")
dir.create(trace_dir, recursive = TRUE, showWarnings = FALSE)

all_sample_ids <- matched_ids
time_cols <- names(dff_results)[grepl("^[0-9]+$", names(dff_results))]

# Color palettes
corr_palette <- colorRampPalette(rev(RColorBrewer::brewer.pal(11, "RdYlBu")))(100)
node_palette <- colorRampPalette(rev(RColorBrewer::brewer.pal(9, "YlGnBu")))(100)

# Helper: binary trace conversion
binary_thresh <- function(trace, threshold_factor = 2, min_frames = 2, max_frames = 150) {
  threshold <- threshold_factor * sd(trace, na.rm = TRUE)
  binary <- as.numeric(trace > threshold)
  filter_spike_duration(binary, min_frames, max_frames)
}

# Helper: Filter bursts that are too short or too long
filter_spike_duration <- function(binary_trace, min_frames = 2, max_frames = 150) {
  rle_trace <- rle(as.logical(binary_trace))
  keep <- rle_trace$values & rle_trace$lengths >= min_frames & rle_trace$lengths <= max_frames
  rle_trace$values <- rle_trace$values & keep
  as.numeric(inverse.rle(rle_trace))
}

for (sample_id in all_sample_ids) {
  clean_sample_id <- gsub("_centroids_valid.csv$", "", basename(sample_id))
  output_dir <- file.path(base_path, clean_sample_id)
  dir.create(output_dir, recursive = TRUE, showWarnings = FALSE)

  df_sample <- dff_results %>% filter(FileID == sample_id)

  time_cols <- names(df_sample)[grepl("^[0-9]+$", names(df_sample))]

  # Second dF/F pass
  df_sample <- df_sample %>%
    rowwise() %>%
    mutate(
      dF_F_second = list({
        trace <- c_across(all_of(time_cols))
        calculate_dff(trace, window = baseline_window, smoothing_k = smoothing_k, percentile = 0.1)
      })
    ) %>%
    ungroup()

  dff_mat <- do.call(rbind, df_sample$dF_F_second)
  stopifnot(nrow(dff_mat) == nrow(df_sample))

  df_sample <- df_sample %>%
    mutate(
      dF_F_scaled = apply(dff_mat, 1, function(x) {
        x_thresh <- ifelse(x < 0.04, 0, x)
        denom <- max(x_thresh, na.rm = TRUE)
        if (denom < 1e-3) denom <- 1e-3
        x_thresh / denom
      }) %>% split(seq_len(nrow(dff_mat)))
    )

  # Binary logic added separately
  dF_F_binary <- lapply(seq_len(nrow(dff_mat)), function(i) {
    trace <- dff_mat[i, ]
    binary_thresh(trace, threshold_factor = threshold_factor, min_frames = 5, max_frames = 150)
  })

  df_sample$dF_F_binary <- dF_F_binary

  dff_scaled <- do.call(rbind, df_sample$dF_F_scaled)
  dff_binary <- do.call(rbind, df_sample$dF_F_binary)

  rownames(dff_scaled) <- df_sample$ObjectNumber
  rownames(dff_binary) <- df_sample$ObjectNumber

  # Save individual trace plots
  for (i in seq_len(nrow(df_sample))) {
    roi_id <- df_sample$ObjectNumber[i]
    trace_raw    <- dff_mat[i, ]
    trace_scaled <- dff_scaled[i, ]
    trace_bin    <- dff_binary[i, ]

    df_trace <- tibble(
      frame = as.numeric(time_cols),
      raw = trace_raw,
      scaled = trace_scaled,
      binary = trace_bin
    )

    p_raw    <- ggplot(df_trace, aes(frame, raw))    + geom_line() + theme_minimal()
    p_scaled <- ggplot(df_trace, aes(frame, scaled)) + geom_line(color = "dodgerblue") + theme_minimal()
    p_binary <- ggplot(df_trace, aes(frame, binary)) + geom_step(color = "firebrick2") + theme_minimal()

    # ggsave(file.path(trace_dir, paste0(clean_sample_id, "_ROI_", roi_id, ".pdf")),
    #        p_raw / p_scaled / p_binary, width = 6, height = 6, device = cairo_pdf)
  }

  # Correlation matrix of binary spike trains
  cor_mat <- tryCatch({
    cm <- cor(t(dff_binary), use = "pairwise.complete.obs")
    cm[!is.finite(cm)] <- 0
    cm
  }, error = function(e) {
    warning(paste("Correlation computation failed for", clean_sample_id, ":", e$message))
    matrix(0, nrow = nrow(dff_binary), ncol = nrow(dff_binary),
           dimnames = list(df_sample$ObjectNumber, df_sample$ObjectNumber))
  })
  
  # plot Heatmaps
  pdf(file.path(output_dir, paste0(clean_sample_id, "_CorrelationMatrix.pdf")), width = 6, height = 6)
  tryCatch({
    pheatmap(cor_mat, clustering_method = "complete",
             main = "Correlation Matrix", border_color = NA, fontsize = 6)
  }, error = function(e) {
    message("Skipped heatmap for ", clean_sample_id, ": ", e$message)
  })
  dev.off()

  
  
# --- Identify synchronized ROIs based on roi_corr_threshold
roi_edge_idx <- which(cor_mat > roi_corr_threshold & upper.tri(cor_mat), arr.ind = TRUE)
roi_high_corr_ids <- unique(c(rownames(cor_mat)[roi_edge_idx[, 1]], rownames(cor_mat)[roi_edge_idx[, 2]]))

# --- Tag synchronized ROIs
df_sample <- df_sample %>%
  mutate(is_high_corr = ObjectNumber %in% roi_high_corr_ids)

# --- Firing stats
firing_stats <- tibble(
  ObjectNumber = df_sample$ObjectNumber,
  is_high_corr = df_sample$is_high_corr,
  firing_events = rowSums(dff_binary == 1 & apply(dff_binary, 2, function(col) c(0, diff(col))) == 1, na.rm = TRUE),
  active_frames = rowSums(dff_binary == 1, na.rm = TRUE),
  total_frames = ncol(dff_binary)
) %>%
  mutate(
    percent_active = 100 * active_frames / total_frames,
    spike_rate = active_frames / total_frames,
    Sample = sample_id,
    firing_rate_per_min = firing_events / (total_frames / (frame_rate * 60)),
    Group = if (sample_id %in% names(genotype_map)) genotype_map[[sample_id]] else "Unknown"
  )
events_list[[clean_sample_id]] <- firing_stats

# --- Summary per sample
sample_summary <- firing_stats %>%
  summarise(
    Sample                     = unique(Sample),
    Total_cells                = n(),
    Total_firing_events        = sum(firing_events, na.rm = TRUE),
    Firing_events_per_min      = mean(firing_rate_per_min, na.rm = TRUE),
    Firing_events_per_min_norm = Total_firing_events / Total_cells,
    Cells_active               = sum(firing_events > 0, na.rm = TRUE),
    Percent_cells_active       = 100 * Cells_active / Total_cells,
    Mean_percent_active_frames = mean(percent_active, na.rm = TRUE),
    Mean_spike_rate            = mean(spike_rate, na.rm = TRUE)
  )

summary_list[[clean_sample_id]] <- sample_summary



# --- Coordinates and node coloring
coords <- df_sample %>%
  filter(!is.na(centroid_x), !is.na(centroid_y)) %>%
  select(ObjectNumber, centroid_x, centroid_y)

coords <- left_join(coords, firing_stats, by = "ObjectNumber") %>%
  mutate(
    Label = as.character(ObjectNumber),
    color_idx = pmin(pmax(round(scales::rescale(spike_rate, to = c(1, 100))), 1), 100),
    color = node_palette[color_idx]
  )

# --- Edge list
edge_list <- which(cor_mat > correlation_threshold & upper.tri(cor_mat), arr.ind = TRUE)
edges <- data.frame(
  from = rownames(cor_mat)[edge_list[, 1]],
  to   = rownames(cor_mat)[edge_list[, 2]],
  corr = cor_mat[edge_list]
)

valid_corr_vals <- cor_mat[upper.tri(cor_mat) & is.finite(cor_mat)]
min_corr <- min(valid_corr_vals, na.rm = TRUE)
max_corr <- 1

if (nrow(edges) == 0 || is.na(min_corr) || min_corr == max_corr) {
  edges$corr_scaled <- integer(0)
  edges$color <- character(0)
} else {
  edges$corr_scaled <- scales::rescale(edges$corr, to = c(1, 100), from = c(min_corr, max_corr))
  edges$corr_scaled <- pmin(pmax(round(edges$corr_scaled), 1), 100)
  edges$color <- corr_palette[edges$corr_scaled]
}

write.csv(edges, file.path(output_dir, "edge_list_with_colors.csv"), row.names = FALSE)

# --- Network plotting wrapper
plot_network <- function(node_colors, edge_colors, legend_title, legend_range, file_name) {
  pdf(file.path(output_dir, file_name), width = 6.9, height = 5)
  layout(matrix(c(1, 2), nrow = 1), widths = c(5, 1.5))
  par(mar = c(4, 4, 2, 1))
  plot(coords$centroid_x, coords$centroid_y,
       pch = 21, bg = node_colors, col = "grey50",
       xlab = "X", ylab = "Y", main = clean_sample_id,
       asp = 1, xlim = c(0, image_dim[1]), ylim = c(0, image_dim[2]))
  for (i in seq_len(nrow(edges))) {
    p1 <- dplyr::filter(coords, Label == edges$from[i])
    p2 <- dplyr::filter(coords, Label == edges$to[i])
    if (nrow(p1) == 1 && nrow(p2) == 1) {
      segments(p1$centroid_x, p1$centroid_y, p2$centroid_x, p2$centroid_y,
               col = edge_colors[i], lwd = 1)
    }
  }
  par(mar = c(4, 2, 2, 4))
  legend_img <- as.raster(matrix(rev(corr_palette), ncol = 1))
  plot(NA, xlim = c(0, 1), ylim = legend_range, type = "n", axes = FALSE, xlab = "", ylab = "")
  rasterImage(legend_img, 0, legend_range[1], 1, legend_range[2])
  axis(4, at = pretty(legend_range), las = 1)
  mtext(legend_title, side = 4, line = 2.5)
  dev.off()
}

# --- Generate plots
plot_network(rep("grey50", nrow(coords)), edges$color, "Correlation", c(min_corr, max_corr), "network_from_centroids.pdf")
plot_network(coords$color, edges$color, "Spike rate", c(min_corr, max_corr), "network_from_centroids_spikerate.pdf")

# --- Save processed data
write.csv(filter(firing_stats, is_high_corr == TRUE),
          file.path(output_dir, paste0(clean_sample_id, "_firing_stats_highcorr.csv")),
          row.names = FALSE)

write.csv(filter(firing_stats, is_high_corr == FALSE),
          file.path(output_dir, paste0(clean_sample_id, "_firing_stats_lowcorr.csv")),
          row.names = FALSE)

df_sample_export <- df_sample %>%
  select(-any_of(c("dF_F_scaled", "dF_F_binary", "dF_F_second")))

write.csv(df_sample_export,
          file.path(output_dir, paste0(clean_sample_id, "_df_processed.csv")),
          row.names = FALSE)


# --- Louvain community detection
g <- igraph::graph_from_data_frame(edges, directed = FALSE)
if (igraph::gorder(g) > 0 && igraph::gsize(g) > 0) {
  comms <- igraph::cluster_louvain(g)
  membership <- membership(comms)
  community_sizes <- sizes(comms)

  membership_df <- tibble(
    ObjectNumber   = names(membership),
    Community      = as.integer(membership),
    Sample         = sample_id,
    CommunitySize  = as.integer(community_sizes[as.character(membership)])
  )

  comm_list[[clean_sample_id]] <- membership_df
}

}


```






# III.  Plotting & Statistical Evaluation
## IIIa. ROI Couting and Event Rates Calculation 
```{r}
## ---------------------------------------------------------------------------
## Plot CellposeSAM / CNER results — incl. events per min⁻¹ (±SEM)
## ---------------------------------------------------------------------------

# --------------------------
# Helpers
# --------------------------
bind_with_sample <- function(lst) {
  imap_dfr(lst, ~ {
    if (!"Sample" %in% names(.x)) .x <- mutate(.x, Sample = .y)
    .x
  })
}

clean_sample_names <- function(s) {
  s %>%
    str_replace("_centroids_valid\\.csv$", "") %>%
    str_replace("_\\d+$", "")
}

# --------------------------
# Bind data from result lists
# --------------------------
events_df <- bind_with_sample(events_list)
comm_df   <- bind_with_sample(comm_list)
comm_df <- comm_df %>% mutate(Label = as.character(ObjectNumber))

# Define normalised_metrics
if (length(summary_list) > 0) {
  normalised_metrics <- bind_with_sample(summary_list) %>%
    mutate(
      Genotype = case_when(
        str_detect(Sample, "Ctrl")  ~ "Ctrl",
        str_detect(Sample, "ASAH1") ~ "ASAH1",
        TRUE                        ~ NA_character_
      )
    )
} else {
  normalised_metrics <- tibble()
}

# Add Genotype to events_df and comm_df
events_df <- events_df %>%
  mutate(
    Genotype = case_when(
      str_detect(Sample, "Ctrl")  ~ "Ctrl",
      str_detect(Sample, "ASAH1") ~ "ASAH1",
      TRUE                        ~ NA_character_
    ),
    ObjectNumber = as.character(ObjectNumber)
  )

comm_df <- comm_df %>%
  mutate(
    Genotype = case_when(
      str_detect(Sample, "Ctrl")  ~ "Ctrl",
      str_detect(Sample, "ASAH1") ~ "ASAH1",
      TRUE                        ~ NA_character_
    ),
    ObjectNumber = as.character(ObjectNumber)
  ) %>%
  left_join(
    events_df %>%
      select(Sample, ObjectNumber, is_high_corr) %>%
      distinct(Sample, ObjectNumber, .keep_all = TRUE),
    by = c("Sample", "ObjectNumber")
  )

# --------------------------
# ROI count and events/min calculation
# --------------------------
events_df <- events_df %>%
  group_by(Sample) %>%
  mutate(
    n_labels = n(),
    events_per_min = firing_events / (total_frames / (frame_rate * 60)),
    events_per_min_norm = events_per_min / n_labels
  ) %>%
  ungroup()

# --------------------------
# Propagate is_high_corr to sample-level
# --------------------------
events_sample_df <- events_df %>%
  group_by(Sample) %>%
  summarise(
    n_total = n(),
    n_high_corr = sum(is_high_corr, na.rm = TRUE),
    prop_high_corr = n_high_corr / n_total,
    .groups = "drop"
  )

if ("Sample" %in% names(normalised_metrics) && nrow(normalised_metrics) > 0) {
  normalised_metrics <- normalised_metrics %>%
    left_join(events_sample_df, by = "Sample", relationship = "many-to-one")
}

comm__sample_df <- comm_df %>%
  left_join(
    events_df %>%
      select(Sample, ObjectNumber, is_high_corr) %>%
      distinct(Sample, ObjectNumber, .keep_all = TRUE),
    by = c("Sample", "ObjectNumber")
  )



# --------------------------
# Inject metadata from Sample
# --------------------------

# Helper to extract matched component
extract_match <- function(x, patterns) {
  matched <- sapply(patterns, function(p) str_detect(x, fixed(p)))
  result <- apply(matched, 1, function(row) {
    if (any(row)) patterns[which.max(row)] else NA_character_
  })
  return(result)
}

# Apply to a dataframe
annotate_sample_metadata <- function(df) {
  df %>%
    mutate(
      Celltype    = extract_match(Sample, celltype),
      Timepoint   = extract_match(Sample, timepoint),
      Genotype    = extract_match(Sample, genotypes),
      Treatment   = extract_match(Sample, treatments),
      SampleLabel = str_c(Genotype, Treatment, sep = "_")
    ) %>%
    relocate(Sample, Celltype, Timepoint, Genotype, Treatment, SampleLabel)
}

# Annotate all three dataframes
events_df <- annotate_sample_metadata(events_df)
comm_df   <- annotate_sample_metadata(comm_df)
normalised_metrics <- annotate_sample_metadata(normalised_metrics)
```


## IIIb. Helper Function - Violin plots with Kruskal-Wallis test 
```{r}
# ----------------------------------------------------------
# Helper: Violin plot with significance stats
# ----------------------------------------------------------
plot_genotype_violin_with_stats <- function(df, metric, ylab, output_prefix, base_dir = ca_results_dir,
                                            add_sem = FALSE, jitter_color = "grey50") {
  require(ggpubr)

  metric_sym <- rlang::sym(metric)

  # Ensure consistent factor levels
  df <- set_samplelabel_factor(df)
  
  # Early exit if <2 unique conditions
  if (length(unique(na.omit(df$SampleLabel))) < 2) {
    message("Skipping plot: Only one condition present in the data subset.")
    return(NULL)
  }

  # Kruskal-Wallis
  kruskal_pval <- kruskal.test(reformulate("SampleLabel", metric), data = df)$p.value

  # Pairwise Wilcoxon
  pw <- pairwise.wilcox.test(df[[metric]], df$SampleLabel, p.adjust.method = "holm")
  pairwise_df <- as.data.frame(as.table(pw$p.value), stringsAsFactors = FALSE) %>%
    dplyr::rename(group1 = Var1, group2 = Var2, p.value = Freq) %>%
    dplyr::filter(!is.na(p.value)) %>%
    dplyr::mutate(
      significance = case_when(
        p.value < 0.001 ~ "***",
        p.value < 0.01  ~ "**",
        p.value < 0.05  ~ "*",
        TRUE            ~ NA_character_
      ),
      y.position = seq(from = max(df[[metric]], na.rm = TRUE) * 1.05,
                       by = max(df[[metric]], na.rm = TRUE) * 0.05,
                       length.out = dplyr::n())
    ) %>%
    filter(!is.na(significance))

  if (add_sem) {
    mean_df <- df %>%
      group_by(SampleLabel) %>%
      summarise(
        mean_val = mean(!!metric_sym, na.rm = TRUE),
        sem      = sd(!!metric_sym, na.rm = TRUE) / sqrt(n()),
        .groups = "drop"
      )
  }

  # Plot
  p <- ggplot(df, aes(x = SampleLabel, y = !!metric_sym, fill = SampleLabel)) +
    geom_violin(scale = "width", trim = FALSE, color = NA, width = 0.85) +
    geom_jitter(width = 0.1, size = 0.5, alpha = 0.5, color = jitter_color) +
    geom_boxplot(width = 0.2, fill = "white", outlier.shape = NA, linewidth = 0.3) +
    scale_fill_manual(values = geno_cols) +
    labs(y = ylab, x = NULL) +
    theme_bw(base_size = 6) +
    theme(panel.grid = element_blank(), legend.position = "none",
          aspect.ratio = 1.5, axis.text.x = element_text(angle = 90, hjust = 0.5))

  if (nrow(pairwise_df) > 0) {
    p <- p + stat_pvalue_manual(pairwise_df,
                                label = "significance",
                                y.position = "y.position",
                                xmin = "group1", xmax = "group2",
                                tip.length = 0.01, size = 2)
  }

  ggsave(file.path(base_dir, paste0(output_prefix, "_violin.pdf")),
         p, width = 3, height = 5, device = cairo_pdf)

  # Return
  kruskal_df <- tibble(
    group1 = "Kruskal-Wallis", group2 = "",
    p.value = kruskal_pval,
    significance = case_when(
      kruskal_pval < 0.001 ~ "***",
      kruskal_pval < 0.01  ~ "**",
      kruskal_pval < 0.05  ~ "*",
      TRUE                 ~ "ns"
    )
  )

  return(list(p = p, stats = bind_rows(kruskal_df, pairwise_df)))
}


```


## IIIc. Run pairwise Wilcoxon test comparisons 
```{r}
# ----------------------------------------------------------
# Run comparisons
# ----------------------------------------------------------

# Split events_df by ROI correlation
events_high <- events_df %>% filter(is_high_corr == TRUE)
events_low  <- events_df %>% filter(is_high_corr == FALSE)

# Per-cell metrics
res_high      <- plot_genotype_violin_with_stats(events_high, "firing_events",   "Firing Events",    "firing_highcorr")
res_low       <- plot_genotype_violin_with_stats(events_low,  "firing_events",   "Firing Events",    "firing_lowcorr")

# Sample-level normalized
res_fire_norm <- plot_genotype_violin_with_stats(normalised_metrics, "Firing_events_per_min_norm",
                                                 "Events · min⁻¹ / ROI", "firing_norm")
res_active_norm <- plot_genotype_violin_with_stats(normalised_metrics, "Mean_percent_active_frames",
                                                   "% Active / ROI", "active_norm")
```


## IIId. Save combined Outputs & Sample-Level Normalized Outputs
```{r}
# ------------------------------------------------------
# Save Combined Outputs (Cell-Pair Correlations, Communities, Events)
# ------------------------------------------------------
write.csv(bind_rows(cor_list),    file.path(ca_results_dataframes, "combined_cell_pairwise_correlations.csv"), row.names = FALSE)
write.csv(bind_rows(comm_list),   file.path(ca_results_dataframes, "combined_cell_community_membership.csv"),  row.names = FALSE)
write.csv(bind_rows(events_list), file.path(ca_results_dataframes, "combined_events_per_min_results.csv"),     row.names = FALSE)

# ------------------------------------------------------
# Save sample-level normalized metrics (already annotated with genotype)
# ------------------------------------------------------
write.csv(normalised_metrics,
          file.path(ca_results_dataframes, "combined_sample_level_normalised_metrics.csv"),
          row.names = FALSE)

```


## IIIe. Sample-Level Normalized Metrics
```{r}
# ---------------------------------------------------------------------------
# Sample‑level normalised metrics from `normalised_metrics`
# ---------------------------------------------------------------------------

# plot low / high corr filtered violin metrics
# firing rate 
p_fire_norm_result <- plot_genotype_violin_with_stats(
  normalised_metrics,
  metric = "Firing_events_per_min_norm",
  ylab = "Events · min⁻¹ / ROI",
  output_prefix = "fire_events_norm",
  base_dir = ca_results_dir
)
p_active_norm_result <- plot_genotype_violin_with_stats(
  normalised_metrics,
  metric = "Mean_percent_active_frames",
  ylab = "% Active / ROI",
  output_prefix = "percent_active_norm",
  base_dir = ca_results_dir
)


# plot low / high corr filtered firing rates 
p_fire_norm_high_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == TRUE),
  metric = "events_per_min_norm",
  ylab = "Events · min⁻¹ / ROI",
  output_prefix = "fire_events_norm_high",
  base_dir = ca_results_dir
)
p_fire_norm_low_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == FALSE),
  metric = "events_per_min_norm",
  ylab = "Events · min⁻¹ / ROI",
  output_prefix = "fire_events_norm_low",
  base_dir = ca_results_dir
)


# plot low / high active frames
p_active_norm_high_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == TRUE),
  metric = "percent_active",
  ylab = "% Active / ROI",
  output_prefix = "percent_active_norm_high",
  base_dir = ca_results_dir
)
p_active_norm_low_results <- plot_genotype_violin_with_stats(
  df = events_df %>% filter(is_high_corr == FALSE),
  metric = "percent_active",
  ylab = "% Active / ROI",
  output_prefix = "percent_active_norm_low",
  base_dir = ca_results_dir
)


# Extract violin plots
p_fire_norm         <- p_fire_norm_result$p
p_active_norm       <- p_active_norm_result$p

p_fire_norm_high    <- p_fire_norm_high_results$p
p_fire_norm_low     <- p_fire_norm_low_results$p

p_active_norm_high  <- p_active_norm_high_results$p
p_active_norm_low   <- p_active_norm_low_results$p

```



## IIIf. Community Size Analysis (Non-parametric) 
```{r}
# ----------------------------------------------------------------------
# Stratify by high/low correlation
# ----------------------------------------------------------------------
comm_high <- comm_df %>% filter(is_high_corr == TRUE)
comm_low  <- comm_df %>% filter(is_high_corr == FALSE)

# ----------------------------------------------------------------------
# Helper to run KW + Wilcoxon + plot
# ----------------------------------------------------------------------
analyze_comm_size <- function(df, label, outfile_prefix) {
  df <- df %>% 
    filter(!is.na(Genotype)) %>%
    set_samplelabel_factor()

  # Skip if only one condition
  if (length(unique(df$SampleLabel)) < 2) {
    message("Skipping analysis: only one condition present.")
    return(list(plot = NULL, stats = tibble()))
  }

  # Kruskal-Wallis test
  kw_pval <- kruskal.test(CommunitySize ~ SampleLabel, data = df)$p.value

  # Pairwise Wilcoxon
  pw <- pairwise.wilcox.test(df$CommunitySize, df$SampleLabel, p.adjust.method = "holm")
  pairwise_df <- as.data.frame(as.table(pw$p.value), stringsAsFactors = FALSE) %>%
    rename(group1 = Var1, group2 = Var2, p.value = Freq) %>%
    filter(!is.na(p.value)) %>%
    mutate(
      significance = case_when(
        p.value < 0.001 ~ "***",
        p.value < 0.01  ~ "**",
        p.value < 0.05  ~ "*",
        TRUE            ~ "ns"
      ),
      y.position = seq(from = max(df$CommunitySize, na.rm = TRUE) * 1.05,
                       by = max(df$CommunitySize, na.rm = TRUE) * 0.05,
                       length.out = dplyr::n()),
      group1 = factor(group1, levels = levels(df$SampleLabel)),
      group2 = factor(group2, levels = levels(df$SampleLabel))
    )

  # Plot
  p <- ggplot(df, aes(x = SampleLabel, y = CommunitySize, fill = SampleLabel)) +
    geom_jitter(color = "grey80", width = 0.1, size = 0.5, alpha = 0.5) +
    geom_violin(scale = "width", trim = FALSE, color = NA, width = 0.85, adjust = 1.5) +
    geom_boxplot(width = 0.2, fill = "white", outlier.shape = NA, color = "black", linewidth = 0.3) +
    scale_fill_manual(values = geno_cols) +
    labs(y = "Community Size", x = NULL, title = label) +
    theme_bw(base_size = 6) +
    theme(legend.position = "none",
          aspect.ratio    = 2,
          panel.grid = element_blank(),
          axis.text.x = element_text(angle = 90, hjust = 0.5))

  if (nrow(pairwise_df) > 0) {
    p <- p + stat_pvalue_manual(pairwise_df,
                                label = "significance",
                                y.position = "y.position",
                                xmin = "group1", xmax = "group2",
                                tip.length = 0.01, size = 2)
  }

  ggsave(file.path(ca_results_dir, paste0(outfile_prefix, ".pdf")),
         p, width = 2.5, height = 3.5, device = cairo_pdf)

  return(list(p = p,
              stats = bind_rows(
                tibble(group1 = "Kruskal-Wallis", group2 = "", p.value = kw_pval,
                       significance = case_when(
                         kw_pval < 0.001 ~ "***",
                         kw_pval < 0.01  ~ "**",
                         kw_pval < 0.05  ~ "*",
                         TRUE            ~ "ns")),
                pairwise_df)))
}

# ----------------------------------------------------------------------
# Run per-group comparisons
# ----------------------------------------------------------------------
res_comm_high <- analyze_comm_size(comm_high, "Community Size (High Corr ROIs)", "community_size_high")
res_comm_low  <- analyze_comm_size(comm_low,  "Community Size (Low Corr ROIs)",  "community_size_low")

# ----------------------------------------------------------------------
# Save combined stats
# ----------------------------------------------------------------------
stats_comm_all <- bind_rows(
  res_comm_high$stats %>% mutate(Source = "high_corr_ROIs"),
  res_comm_low$stats  %>% mutate(Source = "low_corr_ROIs")
)

write.csv(stats_comm_all,
          file = file.path(ca_results_dataframes, "community_size_stats_high_vs_low.csv"),
          row.names = FALSE)



# plots
p_comm_high <- plot_genotype_violin_with_stats(
  df = comm_high,
  metric = "CommunitySize",
  ylab = "# Communities",
  output_prefix = "community_stats_high",
  base_dir = ca_results_dir
)

p_comm_low <- plot_genotype_violin_with_stats(
  df = comm_low,
  metric = "CommunitySize",
  ylab = "# Communities",
  output_prefix = "community_stats_low",
  base_dir = ca_results_dir
)
```




## IIIh. Combined Summary Figure & export of all values 
```{r}
# ---------------------------------------------------------------------------
# Replace ANOVA + t-tests with Kruskal-Wallis + Wilcoxon
# ---------------------------------------------------------------------------

calc_nonparametric_stats <- function(df, metric_col, group_col = "Genotype") {
  df <- df %>%
    dplyr::filter(!is.na(.data[[metric_col]]), !is.na(.data[[group_col]])) %>%
    dplyr::mutate(!!group_col := factor(.data[[group_col]]))

  if (length(unique(df[[group_col]])) < 2) {
    return(tibble(
      Metric = metric_col,
      group1 = "Kruskal-Wallis",
      group2 = "",
      p.value = NA_real_,
      sig_label = "NA"
    ))
  }

  kruskal <- kruskal.test(reformulate(group_col, metric_col), data = df)
  kw_pval <- kruskal$p.value

  pairwise <- pairwise.wilcox.test(df[[metric_col]], df[[group_col]], p.adjust.method = "BH")
  pairwise_df <- as.data.frame(as.table(pairwise$p.value)) %>%
    rename(group1 = Var1, group2 = Var2, p.value = Freq) %>%
    filter(!is.na(p.value)) %>%
    mutate(Metric = metric_col) %>%
    select(Metric, group1, group2, p.value)

  bind_rows(
    tibble(Metric = metric_col, group1 = "Kruskal-Wallis", group2 = "", p.value = kw_pval),
    pairwise_df
  ) %>%
    mutate(sig_label = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01  ~ "**",
      p.value < 0.05  ~ "*",
      TRUE            ~ "ns"
    ))
}

# ---------------------------------------------------------------------------
# Combine and save all nonparametric p-values
# ---------------------------------------------------------------------------
all_pvals_high <- bind_rows(
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == TRUE), "events_per_min_norm"),
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == TRUE), "percent_active"),
  calc_nonparametric_stats(comm_df   %>% filter(is_high_corr == TRUE), "CommunitySize")
) %>% mutate(Source = "high_corr_ROIs")

all_pvals_low <- bind_rows(
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == FALSE), "events_per_min_norm"),
  calc_nonparametric_stats(events_df %>% filter(is_high_corr == FALSE), "percent_active"),
  calc_nonparametric_stats(comm_df   %>% filter(is_high_corr == FALSE), "CommunitySize")
) %>% mutate(Source = "low_corr_ROIs")

all_pvals_combined <- bind_rows(all_pvals_high, all_pvals_low)

write.csv(all_pvals_combined,
          file = file.path(ca_results_dataframes, "calcium_metrics_pvals_high_vs_low.csv"),
          row.names = FALSE)
```
